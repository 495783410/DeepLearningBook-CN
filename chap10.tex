%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% author:dormir_yin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{序列模型：循环网络与递归网络}
\label{chap:10}
循环神经网络（Recurrent neural networks，简称RNN），是一系列处理序列数据（sequential data）的神经网络的统称。前面的章节我们介绍的卷积神经网络是专门来处理格状结构的数据X的，比如说图像。而循环神经网络是用来处理序列的，序列一般表示为：$x^{(1)}...x^{(\tau)}$\footnote{译者注：序列数据可以是时间序列，可以是空间序列，当然也可以是其他形式的的序列。为了便于理解我们假设我们处理的是时间序列，上标对应的是不同的时间点。每个时间点对应的输入向量是$x^{(t)}$}。我们已经知道卷积神经网络在处理长宽都很大的图像和处理小图像模型并没有很大的不同，甚至有的卷积神经网络模型可以处理大小不固定的输入图像。递归神经网络也有类似能力，当递归神经网络处理序列时，并不需要针对不同长度的序列分别设计一个模型。大部分递归神经网络模型可以处理长度变化的输入序列。

在我们介绍循环神经网络之前，首先向大家介绍一下在八十年代机器学习和统计模型领域提出的一个观点：在模型的不同部分共享参数（sharing parameters）。参数共享可以帮助我们扩展模型，让模型去处理不同形式（在这里主要是不同长度的序列）的输入数据。处理序列数据时，如果我们对序列中每个不同的时间点对应的输入元素使用不同的参数，我们就不能处理长度未知的序列，训练的过程中也不能享有统计优势（statistical strength）\footnote{译者注；感觉作者想要表达的意思是序列中每个元素有可能都不是相互独立的，我们如果使用参数共享，可以从整体来提取一些共性的东西。举个例子，如果数据集是从某个分布采样得到的，如果我们对这个数据的整体来处理，反而更容易得到这个分布的信息}。当某个有用的信息在序列的不同位置都有可能出现时，参数共享就变得尤为重要。举个例子：比较两个句子“I went to Nepal in 2009”  和 “In 2009,I went to Nepal.”，如果我们把这两个句子输入机器学习模型，让模型把叙述者去Nepal的年份给提取出来，我们期望模型识别2009作为有用的信息，尽管这个词在这个两个句子中出现的位置不一样：一个是第六个单词，一个是第二个单词。假设我们用前馈神经网络处理句子。一个传统的全连接前馈神经网络只能处理固定长度的句子序列，对序列不同位置或者不同时间点的输入的特征都对应不同的参数，所以它需要学习句子中每个位置所对应的语法规则。与传统的全连接网络不同，循环神经网络在序列的每个时间点所对应的权值参数是一样的，也就是说序列不同位置的输入特征会共享参数。

其实当我们在在1维时间序列（1-D temporal sequence）中使用卷积的时候，也使用了参数共享。序列1维卷积的方法也是时间延迟神经网络（time-delay neural networks）的基础(Lang and Hinton, 1988; Waibel et al., 1989; Lang et al., 1990)。卷积操作允许神经网络在不同的时间上共享权值。但是这个网络特别浅。序列经过卷积之后，它的输出依然是一个序列。输出序列的每个成员是对应位置输入与周围相邻的输入经过某个函数的映射输出。在每个时间点我们都使用了相同的卷积核，这也正体现了前面所提的参数共享的观点。在循环神经网络中，参数共享是以另一种方式实现的。1维卷积只考虑对应位置附近的特征，而在RNN中每个输出序列里的成员都是之前所有的输出经过一个函数映射得到的。也就是说在产生当前时间点输出的时候，我们要考虑之前所有的输出。在计算输出序列每个成员的时候会考虑所有的历史输出，而所采取的映射规则是一致的。如果用计算图来展示这个递归模式，会发现是在一个非常深的计算图中做参数共享。

为了便于理解，我们记RNN处理的序列数据为一系列向量$\bm{x}^{(t)}$。$t$表示向量在序列中的位置，我们可以理解为时间点，范围是从$1$到$\tau$。在实际应用中，RNN通常分批次(minibatch)处理序列，每个minibatch里序列的长度$\tau$可能不一样。为了简化，我们没有在下标中标注批次。实际上，序列中向量的下标不一定要与现实生活中的时间对应起来，有的时候它仅仅表示该成员在序列中的位置。RNN有时也可以处理二维空间数据，比如说图像。在处理与时间有关的序列时，RNN还可以反向处理数据，或者说神经网络在处理序列之前已经将整个序列看了一遍。\footnote{这段现在解释起来有点难度，在看到双向循环神经网络就会理解了。} 

这章会对我们之前学过的计算图进行扩展，我们会看到带有环状结构的计算图。这个环表示当前时间点会作为一个变量影响未来时间点的值。这类计算图可以帮助我们定义循环神经网络。我们随后会展示用不同的方式构建，训练和使用循环神经网络。

如果你想了解更多和循环神经网络有关的内容，可以看一下Graves (2012)。

\section{计算图的展开}
\label{sec:10.1}
计算图可以形象的展示一系列计算操作，在神经网络中计算题可以清晰的展示如何将输入和模型参数映射成为输出和损失。可以从\ref{sec:6.5.1} 看一下更详细的介绍。在这一节我们将会解释如何将递归或者循环计算展开成具有重复结构的计算图，这个展开的计算题是一个链式结构。在展开过程中我们或看到如何在一个深的网络结构中使用参数共享。
举个例子，考虑一下典型的动态系统:
\begin{equation}
\bm{s}^{(t)} = f(\bm{s}^{(t-1)};\theta)
\label{form:10.1}
\end{equation}

其中$\bm{s}^{(t)}$表示当前系统状态。
等式\ref{form:10.1}是递归的因为$t$时刻状态$\bm{s}^{(t)}$的定义会使用到$t-1$时刻的状态$\bm{s}^{(t-1)}$定义，状态定义的方式是一样的，所以这个式子是递归的。

对于一个有限长的序列，假设他的长度是$\tau$，我们只需要重复使用$\tau-1$次上面的式子就可以把计算图展开。比如说我们要展开式子$\tau =3$，我们可以得到
\begin{eqnarray}
\bm{s}^{(3)} & = & f(\bm{s}^{(2)};\theta) \\
& = & f(f(\bm{s}^{(1)};\theta);\theta)
\end{eqnarray}
通过重复的调用等式\ref{form:10.1}来展开这个等式，我们可以得到一个不包含递归部分的表达式。这个表达式就可以用传统无环计算图来表示。式子\ref{form:10.1}对应展开的计算图可以看 
图\ref{fig:10_1}。
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_1.PNG} 
   \caption{这是一个展开的计算图，描述的是等式 定义的一个动态系统。每个节点代表不同时刻$t$的状态，函数$f$将$t$时刻的状态映射到$t+1$时刻的状态，每个$f$所使用的参数是一样的。}
   \label{fig:10_1}
\end{figure}

我们现在看另一个例子，我们考虑另一个动态系统，这个系统每次会接收一个外部的信号$\bm{x}^{(t)}$,
\begin{equation}
\bm{s}^{(t)} = f(\bm{s}^{(t-1)},\bm{x}^{(t)};\theta)
\label{form:10.4}
\end{equation}
我们可以看到当前状态包含所有之前时间点的历史信息。

循环神经网络可以用不同的方式来构建。我们所使用的函数几乎都可以看作是前馈神经网络（输入经过函数映射到输出），如果在函数外面再引入循环结构，那么就可以被看作循环神经网络。

很多神经网络用等式\ref{form:10.5} 或者类似式子来定义它们隐藏单元的值。为了这个状态是神经网络的隐藏单元，我们对式子\ref{form:10.4}进行了修改，用变量$h$来表示这个状态:
\begin{equation}
\bm{h}^{(t)} = f(\bm{h}^{(t-1)},\bm{x}^{(t)};\theta)
\label{form:10.5}
\end{equation}
这个式子对应的图是\ref{fig:10_2}典型的RNN会增加一个额外的结构，比如说输出层，从隐藏状态$h$中提取信息来作出预测。
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_2.PNG} 
   \caption{一个没有输出的循环神经网络结构。网络会从输入$x$中提取信息，然后整合到隐藏状态$h$中，$h$会被传递下去作为下一个时间点输入的一部分。(左边)未展开的带环计算图。黑色的方块表示一个单位时间的延迟。(右边) 展开的计算图,本质和右边带环的图是一样的，每个节点对应一个特定的时间点。}
   \label{fig:10_2}
\end{figure}

等式\ref{form:10.5}可以用两种不同的方式来理解，我们用计算图来来帮助我们理解这两种思路。第一种可以看作是未展开的计算图，只包含有一个节点，我们甚至可以在现实中用物理实现这个模型。参考生物意义上的神经网络。在这个情况下，图中包含有一个环，它会实时的对网络进行操作。就像图\ref{fig:10_2} 左边的那个图一样。在本章中，我们在环中加了一个黑色的小方块来表示某个交互操作延后了一个时间单位，从$t$时刻的状态延后到$t+1$\footnote{译者注；这个地方不太好翻译，大家对照着图\ref{fig:10_2}去理解。}。另一种是用一个展开的计算图来描述RNN。图中有很多的节点，每个节点表示一个变量，这样每个时间点都对应一个变量来表示在那个时间点的状态。就像图\ref{fig:10_2}右边的那样。展开就是把图\ref{fig:10_2}左边带环的图映射到右边有重复结构的计算图的形式。展开的图的大小取决于序列的长度。

我们可以用函数$g^{(t)}$来表示经过$t$步展开之后的式子：
\begin{eqnarray}
\bm{h}^{(t)} & = & g^{(t)}(\bm{x}^{(t)},\bm{x}^{(t-1)},...,\bm{x}^{(2)},\bm{x}^{(1)}) \\
& = & f(\bm{h}^{(t-1)},\bm{x}^{(t)};\theta)
\end{eqnarray}

函数$g^{(t)}$把整个历史序列$(\bm{x}^{(t)},\bm{x}^{(t-1)},...,\bm{x}^{(2)},\bm{x}^{(1)})$都作为输入，输出为当前状态。但是展开的循环结构允许我们将函数用$f$来展开，我们只要递归调用函数$f$就可以得到函数$bm{h}$。展开过程有两个主要的优势:
\begin{enumerate}
\item 无论序列有多长，模型的输入变量的维度是一致的，因为从一个状态转到另一个状态的映射是固定的。而不是说把一个个长度不定的历史序列映射到一个个状态。
\item 我们每一步都可以使用一样的转换函数$f$，函数$f$的参数也可以共享。
\end{enumerate}

正因为这两个优势，我们可以只训练一个模型$f$, 这样我们不需要考虑序列的长度，每一步我们使用相同的$f$,不需要对所有可能的时间点都学习一个模型$g^{(t)}$。使用一个可以共享参数的模型能够允许我们将模型泛化到任意长度的序列中，即使某个序列的长度没有在训练集中出现。相比较不使用参数共享的模型，我们的模型在训练的时候只需要少量的训练数据。

循环图和展开图有不同的用处。循环图是简洁的。展开的计算图可以向我们展示每一步的计算，也可以展示信息流动的路径：如何沿着时间方向流动的（计算输出和损失），如何反着流动(计算梯度)。

\section{循环神经网络}
\label{sec:10.2}
在\ref{sec:10.1}我们介绍了计算图的展开和参数共享的思想。现在我们可以设计其他更复杂的递归神经网络模型。
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_3.PNG} 
   \caption{这个计算图是用来展示如何计算循环神经网络训练过程中损失函数的值。这个循环神经网络会将输入序列$\bm{x}$映射到一个输出序列$\bm{o}$。损失函数$L$会在训练过程计算预测序列和对应的真实序列之间的距离。当我们使用softmax作为输出，我们假设$\bm{o}$是未归一化的对数概率。损失函数会首先计算$\hat{y}=softmax(\bm{o})$，然后将它和真实值$\bm{y}$进行比较。RNN使用权重矩阵$U$构建的映射关系，将输入映射到隐藏状态，隐藏状态到隐藏状态的映射是权重矩阵$W$来构建的，隐藏状态到输出状态的映射是有权重矩阵$V$来构建的。等式\ref{form:10.8}定义了这个模型的前向传播表达式。(左边)RNN模型和损失函数计算。(右边)将左边的计算图展开了，每个节点对应着一个特定的时间节点。}
   \label{fig:10_3}
\end{figure}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_4.PNG} 
   \caption{在这个递归神经网络模型中，递归部分是输出指向下个时间点对应的隐藏状态的反馈。在每个时间点$t$, 输入是$\Vx_t$，隐藏层的节点是$\Vh^{(t)}$，输出是$\Vo^{(t)}$，真实值是$\Vy^{(t)}$，损失是$L^{(t)}$。(左边)带环的计算图。(右边)展开的计算图。这个模型和图\ref{fig:10_3}表示的RNN模型相比有一些局限（只能模拟很少的一部分函数），图\ref{fig:10_3}代表的模型可以将任意的历史信息存储到隐藏状态$\Vh$中，让后将$\Vh$传给之后的隐藏状态。这个图中的RNN需要把一个特制的输出值放进$\Vo$中，只有$\Vo$会被传递给之后的节点。无法直接从$\Vh$传递信息给后面的节点。之前的隐藏节点$\Vh$不能直接和当前的隐藏节点相连，只能通过它产生的输出来连接下一个隐藏节点。除非$\Vo$是一个维度非常大的向量，可以存储很多的信息，否则$\Vo$或丢失掉很多重要的历史信息。这就造成了这个模型有一些局限，但是这个模型很容易训练，因为每一步可以独立的进行训练，所以在训练过程中我们可以使用并行计算，这个我们在\ref{sec:10.2.1}中进行了介绍。}
   \label{fig:10_4}
\end{figure}

下面我们介绍了典型循环神经网络一些重要设计模式的例子：
\begin{itemize}
\item 循环神经网络每一时间点产生一个输出，隐藏单元之间互相联系，如图\ref{fig:10_3}。
\item 循环神经网络每一时间点产生一个输出，隐藏单元之和前一个时间点的输出有联系,如图\ref{fig:10_4}。
\item 循环神经网络的隐藏单元互相联系，在处理完整个序列之后常数一个输出,如图\ref{fig:10_5}。

\end{itemize}
本章主要介绍图\ref{fig:10_3}展示的模型。

图\ref{fig:10_3}和\ref{form:10.8}展示的循环神经网络模型是图灵完备的，任何可以被图灵机计算的函数也能够被一个有限大小的循环神经网络来计算。在一些时间步之后，计算RNN的输出的算法复杂度与和图灵机用的时间步的长度是线性渐进关系，和输入的长度也是线性渐进关系(Siegelmann and Sontag, 1991; Siegelmann, 1995; Siegelmann and Sontag, 1995;Hyotyniemi, 1996)。图灵机能计算的函数是离散的，所以这些结果注重精确的实现函数，而不是近似。当我们把RNN作为一个图灵机使用，输入为二进制序列，输出必须先离散化来使输出变成二进制。用一个单独特制有限大小的RNN在这个设置下计算所有可以模拟图灵机的函数是可行的(Siegelmann andSontag (1995) use 886 units)。图灵机的输入是一个特制的需要计算函数，所以一个能够模拟图灵机的RNN可以解决所有问题。理论上的RNN可以模拟无线堆栈，只要将他的激活值和权重由无限精度的有理数来表示。\footnote{这段感觉主要讲RNN是图灵完备的，那几篇论文主要证明了RNN和图灵机的等价。如果没学过计算复杂性理论我觉得没必要看了。后面讲了无限堆栈，不知道和数据结构里的堆栈有啥联系，不是很懂。大家这段可以略过，不影响之后的内容。}

我们现在已经得到了图\ref{fig:10_3}描述的RNN前向传播的公式。但是这个图并没有表明隐藏单元使用的激活函数。这里我们假设他使用的激活函数是是双曲正切（tanh）。这个图也没有展示输出和损失函数的形式。这儿我们假设输出是离散的，因为RNN经常被用来预测单词和字母。一个很自然表示离散变量的方式是把输出$\Vo$看作是一个对于离散变量，每个可能的值赋予一个未被归一化的对数概率。然后我们就可以通过softmax操作作为来处理刚刚的数据，这样就可以得到归一化之后的输出概率$\hat \Vy$。前向传播从初始值$\Vh^{(0)}$开始。然后接下来每一步，从$t= 1$到$t = \tau$我们使用下面的公式递归来完成:
\begin{align}
\label{eq:108a}
 \Va^{(t)} &= \Vb + \MW \Vh^{(t-1)} + \MU \Vx^{(t)}, \\
  \Vh^{(t)} &= \tanh(\Va^{(t)} ), \\
  \Vo^{(t)} &= \Vc + \MV \Vh^{(t)}, \\
  \hat \Vy^{(t)} &= \text{softmax}(\Vo^{(t)}),
\end{align}
这些公式里面的参数有偏置向量$\Vb$和$\Vc$，以及权重矩阵$\MU$、$\MV$和$\MW$，这些权重矩阵分别对应三个函数映射。输入到隐藏状态，隐藏到输出状态，隐藏到隐藏状态。这个例子展示了循环神经网络如何将输入序列映射到相同长度的输出序列。计算损失时我们得根据输入序列$\Vx$，和对应的目标序列$\Vy$，然后我们把每个时间点对应的损失都加起来。比如说，如果损失$L^{(t)}$我们选给定 $\Vx^{(1)}, \dots, \Vx^{(t)}$情况下$\Vy^{(t)}$的下负对数似然（negativelog-likelihood），那么：
\begin{align} \label{eq:1012L}
 & L\big( \{ \Vx^{(1)}, \dots, \Vx^{(\tau)} \}, \{ \Vy^{(1)}, \dots, \Vy^{(\tau)}  \} \big) \\
 & = \sum_t L^{(t)} \\
 & = - \sum_t \log p_{\text{model}} \big(  y^{(t)} |  \{ \Vx^{(1)}, \dots, \Vx^{(t)} \} \big) ,
\end{align}
其中$p_{\text{model}} \big(  y^{(t)} |  \{ \Vx^{(1)}, \dots, \Vx^{(t)} \} \big) $是需要我们计算模型的输出$\hat \Vy^{(t)}$中对应目标值$y^{(t)}$的信息\footnote{译者注： 此处不确定，感觉原文有误，entry应该为entropy，希望在之后的校对中解决这个问题}。计算参数在该损失函数中的梯度计算很复杂。计算梯度的时候我们需要先从左到右使用前向传播算法，正如展开图\ref{fig:10_3}一样，接着从右到左使用后向传播算法。时间复杂度是$\CalO(\tau)$, 而且这个过程不能通过并行计算来提速。因为前向计算时是有顺序的。必须要先计算好一个然后再计算下一个。前向计算的所有结果必须先储存起来，因为在后向计算的时候会被用到，后向计算用完之后就可以销毁了。所以算法的存储复杂度也是$\CalO(\tau)$，当我们把后向传播算法应用搭配RNN的展开图中的时候，复杂度为$\CalO(\tau)$，我们叫这个算法的名字是随着时间后向传播算法（back-propagation through time）简称 BPTT。这个算法的细节会在\ref{sec:10.2.2}讲。这个在隐藏状态之间存在递归的网络是非常强大的，但是训练需要很多时间。有没有可以替代的方案呢?

\subsection{老师驱动(Teacher Forcing)以及有循环输出的神经网络}
\label{sec:10.2.1}
如果神经网络的递归部分存在于每一步的输出和下一步的隐藏单元的连接中，网络的能力会比较小。因为它缺少隐藏状态之间的递归联系。举个例子，它不能模拟一个通用图灵机。由于网络结构缺少隐藏层之间的联系，它需要输出单元能够将所有的历史信息都保存起来，然后网络可以用来预测未来信息。因为输出单元主要是用来匹配训练集的目标的，而不是用来从历史输入中提取有用的信息。除非使用者已经知道如何描述系统的所有状态，然后把他作为训练集目标输出的一部分，这个时候存储有用信息和输出尽量与目标相等这两个任务就重合了。把隐藏单元到隐藏单元的递归关系去掉的好处是: 如果损失函数是基于比较$t$时刻的预测值和训练目标的时候，每个时间点对应的部分都是互不影响的。所以训练是可以并行进行，每一步的梯度是独立求解的。没有必要先去计算前面步骤的输出，因为训练集提供了我们需要输出的理想值\footnote{译者注：此时目标$\Vy$中提供了所有有用的信息，而我们的预测$\hat{\Vy}$肯定不会有目标$\Vy$这么完美}。
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_5.PNG} 
   \caption{这个展开的计算图只有在序列的末尾才有一个的输出。这类神经网络模型主要用来对序列进行提取抽象特征，将之表示为一个固定维度的向量（embedding）用于之后的处理。在这个模型中可能每个序列会有一个目标向量来和这个输出进行比较，也有可能使用后向传播算法的时候有梯度信息从之后的模块中流过来。}
   \label{fig:10_5}
\end{figure}

具有从输出到模型递归联系的的模型，可以通过教师驱动(teacher forcing)来训练\footnote{译者注：有的论文中也喜欢叫oracle}。教师驱动是从最大似然准则衍生出来的，在训练过程中，模型会接受到输出的真实值$y^{(t)}$作为$t+1$的输入。我们可以通过下面只有两个时间步序列的例子来了解一下这个方法。条件最大似然准则是：
\begin{align}
 &\log p(\Vy^{(1)},\Vy^{(2)} ~|~ \Vx^{(1)}, \Vx^{(2)} ) \\
 &= \log  p(\Vy^{(2)} ~|~\Vy^{(1)}, \Vx^{(1)}, \Vx^{(2)} )  + \log p(\Vy^{(1)} ~|~ \Vx^{(1)}, \Vx^{(2)}) .
\end{align}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_6.PNG} 
   \caption{ 这个图展示了教师驱动。教师驱动是一种训练技巧，常常用于输出与下一个时间点的隐藏状态有连接的循环神经网络。(左边)在训练过程中我们将$t$时刻对应的真实值$\Vy^{(t)}$ 作为输入传递给下一个隐藏节点$\Vh^{(t+1)}$。(右边)当我们真正使用这个模型或者测试的时候，真实值是不知道的，我们就会用我们模型自己做的预测值$\Vo^{(t)}$来近似代替$\Vy^{(t)}$传递给下一个隐藏状态。这里就形成了一个反馈}
   \label{fig:10_6}
\end{figure}
在这个例子中，我们看到在$t=2$的时刻，模型希望通过训练来实现在给定当前的序列$\Vx$和之前的训练集的输出$\Vy$条件下，最大化$\Vy^{(2)}$的条件概率。最大化似然是在训练过程中实现的，而不是将输出反馈给模型来实现的。我们在训练过程中不会将输出反馈给模型。我们需要传递给模型目标值，来告诉模型正确的输出是什么样的。这在图\ref{fig:10_6}做了说明。

这个模型虽然不是很强大，但是它没有隐藏到隐藏之间的联系，我们算梯度的时候就不需要像传统的RNN一样，根据时间一步一步进行后向运算。教师指导在模型拥存在隐藏节点到隐藏节点关系的时候也会被应用到模型中，只要模型当前时间点的输出与后面某个需要计算值之间存在联系我们就可以使用。但是，只要隐藏单元是历史信息的函数，我们就需要使用BPTT算法。一些模型会同时用教师指导和BPTT来进行训练。

如果之后模型会在\textbf{开环}(open-loop)模式下使用，此时网络输出（如果输出是一个概率分布或者服从某个概率分布，也有可能是输出分布样本）会被作为输入重新反馈到模型中，如果我们使用严格的教师指导来进行训练，这个时候会产生一些问题。在这种情况下，网络在训练过程中看到的输入的种类会和测试的时候看到的输入的分布会非常不一样\footnote{译者注 原文是kind，本人感觉应该是输入分布的不同，校对者来做最终的决定}。一种缓和这个问题的方式是同时使用教师指导下的输入（目标值）和模型自己产生的输出作为输入，比如说通过展开的输入到输出联系预测未来几个时间点正确的目标值。在这个方式下，网络可以学习那些没有在训练过程中见到过输入（比如说那些通过自由运转模式自己生成的输出作为的输入）以及学习如何将这个状态映射到是能够使网络在未来的几步产生合适输出。另一个方法(Bengioet al., 2015b)来解决训练过程中的输入和测试集输入不同的方式是随机选择生成的输出作为输入或者用目标值作为输入。这个方法使用了一个课程学习策略（curriculum learning strategy），它会逐渐的使用更多的生成的值来取代目标值作为输入。

\subsection{计算循环神经网络的梯度}
\label{sec:10.2.2}
计算循环神经网络的梯度是很直接的。只需要将\ref{sec:6.5.6}介绍的后向算法稍微修改一下就可以在展开图中使用，来计算梯度。通过后向算法得到的梯度随后可以被用于任何基于梯度的优化方法来训练RNN。

为了方便大家理解BPTT算法如何运作，我们提供了一个例子来展示如何通过BPTT来给RNN计算梯度(等式\ref{eq:108a}和\ref{eq:1012L})。我们计算图的节点包括参数$\MU,\MV,\MW, \Vb$和$\Vc$，以及以$t$为索引的序列节点$\Vx^{(t)}, \Vh^{(t)},\Vo^{(t)}$和$L^{(t)}$。对于每一个节点$\TSN$，我们需要递归地计算梯度$\nabla_{\TSN} L$，计算过的梯度会传递给计算图中前面的节点，在计算前面节点的梯度的时候会用到后面节点对应的梯度。递归从损失函数对应的那个节点开始：
\begin{align}
 \frac{\partial L}{\partial L^{(t)}} = 1.
\end{align}
在这个导数表达中，为了是最终的预测值$\hat{\Vy}$为归一化的概率向量，我们假定输出$\Vo^{(t)}$ 被传递给softmax函数。我们还假设损失函数为给定输入之后对目标值的负对数似然估计。对$t$时刻输出在损失函数上的梯度$\nabla_{\Vo^{(t)}} L$，它的第$i$项可以这样计算：
\begin{align}
 (\nabla_{\Vo^{(t)}} L)_i =  \frac{\partial L}{\partial o_i^{(t)}} 
 =  \frac{\partial L}{\partial L^{(t)}}  \frac{\partial L^{(t)}}{\partial o_i^{(t)}}  
 = \hat y_i^{(t)} - \mathbf{1}_{i,y^{(t)}}.
\end{align}
之后我们可以从序列的最后一项开始，逐步向前进行后向算法。最后的时间步$\tau$对应的项 $\Vh^{(\tau)}$只有$\Vo^{(\tau)}$作为后续节点，因此这个序列计算起来很简单：
\begin{align}
 \nabla_{\Vh_{(\tau)}} L = \MV^\top \nabla_{\Vo^{(\tau)}} L.
\end{align}
我们接着可以迭代地从时刻$t=\tau-1$到$t=1$进行后向计算，来随着时间后向传播梯度。$\Vh^{(t)}(t < \tau)$同时具有$\Vo^{(t)}$和$\Vh^{(t+1)}$两个后继节点。所以它的梯度用如下的方式进行计算：
\begin{align}
  \nabla_{\Vh_{(\tau)}} L = \Big( \frac{\partial \Vh^{(t+1)}}{ \Vh^{(t)}}  \Big)^\top(\nabla_{\Vh^{(t+1)}} L) 
  + \Big( \frac{\partial \Vo^{(t)}}{ \Vh^{(t)}}  \Big)^\top (\nabla_{\Vo^{(t)}} L) \\
  = \MW^\top (\nabla_{\Vh^{(t+1)}} L) \text{diag} \Big( 1 - (\Vh^{(t+1)})^2 \Big) 
  + \MV^\top \nabla_{\Vo^{(\tau)}} L,
\end{align}
其中$\text{diag} \Big( 1 - (\Vh^{(t+1)})^2 \Big) $ 表示一个包含元素$1 - (h_i^{(t+1)})^2$的对角矩阵。这是时刻$t+1$对应隐藏单元$i$（激活函数为双曲正切）对应的雅可比矩阵（Jacobian）。

一旦获得了计算图中的内部节点的梯度，我们可以获得参数节点的梯度。因为这些参数是在所有的时间节点是被共享的。在对这些参数进行求导计算的时候是要多加小心。我们需要根据\ref{sec:6.5.6}中介绍的bprop实现的方式，来计算计算图中每条边对梯度的贡献。但是，微积分中的$\nabla_{\MW} f$算子，计算$\MW$对于$f$的贡献时将计算图中\emph{所有}边都考虑进去了。为了消除这个歧义，我们引入哑变量$\MW^{(t)}$，他是$\MW$的副本，但是只在$t$时刻使用。我们可能用$\nabla_{\MW^{(t)}}$来表示，$t$时刻对应的权重对梯度的贡献。

通过这些符号，其他参数的梯度可以这样计算：
\begin{align}
 \nabla_{\Vc} L &=  \sum_t \Big( \frac{\partial \Vo^{(t)}}{\partial \Vc} \Big)^\top \nabla_{\Vo^{(t)}} L 
 = \sum_t \nabla_{\Vo^{(t)}} L ,\\
 \nabla_{\Vb} L &= \sum_t \Big( \frac{\partial \Vh^{(t)}}{\partial \Vb^{(t)}} \Big)^\top \nabla_{\Vh^{(t)}} L 
 = \sum_t \text{diag} \Big( 1 - \big( \Vh^{(t)} \big)^2 \Big)  \nabla_{\Vh^{(t)}} L  ,\\
 \nabla_{\MV} L &= \sum_t \sum_i \Big( \frac{\partial L} {\partial o_i^{(t)}}\Big) \nabla_{\MV} o_i^{(t)} 
 = \sum_t (\nabla_{\Vo^{(t)}} L) \Vh^{(t)^\top},\\
 \nabla_{\MW} L &= \sum_t \sum_i \Big( \frac{\partial L} {\partial h_i^{(t)}}\Big) 
 \nabla_{\MW^{(t)}} h_i^{(t)} \\
&= \sum_t \text{diag} \Big( 1 - \big( \Vh^{(t)} \big)^2 \Big) ( \nabla_{\Vh^{(t)}} L) \Vh^{(t-1)^\top} ,\\
 \nabla_{\MU} L &= \sum_t \sum_i \Big( \frac{\partial L} {\partial h_i^{(t)}}\Big) 
 \nabla_{\MU^{(t)}} h_i^{(t)} \\
&= \sum_t \text{diag} \Big( 1 - \big( \Vh^{(t)} \big)^2 \Big) ( \nabla_{\Vh^{(t)}} L) \Vx^{(t)^\top} 
\end{align}

我们不需要在训练的时候计算损失函数对输入$\Vx^{(t)}$的梯度，因为梯度是用于更新参数的，而输入节点的后面没有参数节点了，所以没有必要计算他们对应的梯度。

\subsection{把循环神经网络看作有向图模型}
我们目前介绍的循环神经网络的例子中，损失函数$L^{(t)}$是训练目标$\Vy^{(t)}$和输出$\Vo^{(t)}$之间的交叉熵。循环神经网络也是前馈网络的一种，所以我们其实可以使用任意的的损失函数。我们需要根据任务来寻找合适的损失函数。作为一个前馈网络，我们通常希望RNN的输出可以被看作是概率分布，随后我们可以根据输出分布来与目标之间的交叉熵来定义损失。举个例子，前馈神经网络中，如果输出是高斯单元的话，均方误差就是交叉熵损失。

但我们使用一个预测目标时的对数似然训练过程中的目标函数，比如说等式\ref{eq:1012L}，我们在训练RNN的时候，在给定前面的序列元素的条件之下，估计下一个序列元素的条件分布。这个可以看作最大化对数似然
\begin{align}
 \log p(\Vy^{(t)} \mid \Vx^{(1)},\dots, \Vx^{(t)}),
\end{align}
或者，如果模型包含从某一结点输出到下一节点的联系时，
\begin{align}
 \log p(\Vy^{(t)} \mid \Vx^{(1)},\dots, \Vx^{(t)},\Vy^{(1)},\dots, \Vy^{(t-1)} ).
\end{align}
把整个输出序列$\Vy$的联合概率分解为基于一系列单步的概率预测相乘的形式，可以帮助我们获得整个序列的联合概率分布。当我们不再把$\Vy$值作为预测下一个元素的条件时，这个有向图模型不再包含任何从过去$\Vy^{(i)}$到当前$\Vy^{(t)}$的边。在这个情况下，当给定序列$\Vx$的值的作为条件的时候，输出$\Vy$里面的元素是条件独立的。但我们反馈给网络当前真实的$\Vy$值（不是对应的预测值，而是真正观测到或生成的值），那么有向图模型包含了所有的从过去$\Vy^{(i)}$到当前$\Vy^{(t)}$的边。

举一个简单的例子，我们想象一个RNN模型只处理一个标量随机变量序列$ \SetY = \{\RSy^{(1)},\dots,\RSy^{(\tau)}\}$，没有额外的输入$\RSx$。里面的每个值是随机变量，没有其他额外的输出x。$t$时刻的输入时$t-1$时刻的输出。RNN为这$\RSy$个变量定义了一个有向图模型。我们使用链式法来利用条件概率参数化这些观测值的联合分布：
\begin{align}
 P(\SetY) = P(\RVy^{(1)},\dots,\RVy^{(\tau)}) = \prod_{t=1}^{\tau}P(\RVy^{(t)} \mid \RVy^{(t-1)},\RVy^{(t-2)},
 \dots,\RVy^{(1)}),
\end{align}
当$t=1$时，竖杠右侧显然为空。因此，根据这样一个模型，这一系列值$\{y^{(1)},\dots,y^{(\tau)} \}$的负对数似然为
\begin{align}
 L = \sum_{t} L^{(t)},
\end{align}
其中, 
\begin{align}
 L^{(t)} = -\log P(\RSy^{(t)} = y^{(t)} \mid y^{(t-1)},y^{(t-2)}, \dots, y^{(1)}).
\end{align}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_7.PNG} 
   \caption{ 序列$\RSy^{(1)},\RSy^{(2)},\dots,\RSy^{(t)},\dots$对应的全连接的图模型。之前的观测值$\RSy^{(i)}$，会影响未来的观察值$\RSy^{(t)}(t>i)$的条件分布。直接参数化这个图模型会有点麻烦，因为随着输入的增加参数也会大量增加。RNN也是一个全连接结构，但是它使用了较少的参数,可以看一下图\ref{fig:10_8}来进行一下对比。}
   \label{fig:10_7}
\end{figure}

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_8.PNG} 
   \caption{ 展示了RNN代表的图模型的状态变量，尽管输入到状态变量的映射是一个固定的函数，但是它可以帮助我们使用较少的参数来参数化\ref{form:10.5}我们的模型。序列中的每一个步骤都使用了相同的结构（每个节点具有相同的输入维度），并且这些结构每一步共享它们的参数。}
   \label{fig:10_8}
\end{figure}

图模型中的边表示的是变量之间的依赖关系。很多图模型期望可以通过省略一些没有很强交互性的边来换取统计和计算的效率。举个例子，通常我们会假设图模型具有马尔科夫性。这个时候图中只包含从$\{ \RSy^{(t-k)}, \dots, \RSy^{(t-1)}\}$到$\RSy^{(t)}$的边，而不是包含所有从历史节点到当前结点的边。但是在有的情况下，我们相信所有的历史的输入对序列的下一个元素都有影响。当我们认为$\RSy^{(t)}$的分布可能取决于过去的某个节点$y^{(i)}$，且无法通过$y^{(t-1)}$获得$y^{(i)}$的信息时，RNN就可以大显身手。

当我们用图模型的解释RNN的时候，我们可以把RNN看作是一个全连接的图模型。这是表示任何一对的$y$之间都有依赖关系。图\ref{fig:10_8}是$y$值之间具有全连接结构的图模型。
我们在学习这个RNN代表的完全图的时候通过边缘化\footnote{译者注：概率统计有介绍，把某个随机变量的所有值累加起来}的方式消去了模型中的隐藏单元$\Vh^{(t)}$。

当我们把RNN看作是一个图模型的时候，我们可以把隐藏单元$\Vh^{(t)}$看作是随机变量\footnote{给变量的父节点之后，那些变量的条件分布就确定了。
尽管设计具有这样隐藏单元具有确定分布的图模型是很少见的，但这是完全合理的。}。
在RNN对应的图模型中包括隐藏单元，说明我们可以非常有效的参数化观测的联合分布。
假设我们用表格（tabular representation）来表示某些离散变量上任意的联合分布，在表格中，任意的离散变量的组合都对应着一个小格子，里面的值代表这个组合发生的概率。
如果$y$可以取$k$个不同的值，表格将会有$\CalO(k^\tau)$个参数。
但是RNN由于参数共享，它的参数数目为$\CalO(1)$，是序列长度的函数\footnote{译者注：此处存疑，感觉是作者笔误，复杂度应该为常数}
我们可以调节参数数量来控制RNN的模型容量，但参数数目不会随着序列长度增加而增加。
等式\ref{eq:105h}展示了RNN通过递归掉用相同的函数$f$以及在每个时间点的使用相同参数$\Vtheta$，有效地参数化的变量之间的联系，即使这些变量之间的距离可能很长。
可以通过图\ref{fig:10_8}加深我们对RNN所代表的图模型的理解。
在图模型中，如果我们给定$\Vh^{(t)}$节点，这个节点是过去和未来的中间节点，这个时候，过去和未来就相互独立了。
过去的某个变量$y^{(i)}$可以通过$\Vh$来影响未来的某个变量$y^{(t)}$。
在这个图模型中，我们对每个时间步都使用相同的条件概率分布，这样就可以高效地参数化模型，并且当观察到全部变量时，可以高效地评估联合分布分配给所有变量的概率\footnote{译者注：此处不是很清晰}。

虽然我们可以高效地参数化图模型，某些操作在计算上依然具有挑战性。
比如说，难以预测在序列中间缺少的值。

虽然循环神经网络的参数数目并不多，但是参数\emph{优化}可能会变困难。

我们之所以在循环神经网络中使用的参数共享，是因为我们假设相同参数可用于序列不同的位置。
同样的，这个也是假设在给定时刻$t$的变量的条件下，在时刻$t +1$变量的条件概率分布是固定的stationary，这意味着序列不同位置的变量之间的关系并不依赖于位置$t$。
原则上，可以使用$t$作为每个时间步的额外输入，并让学习器在发现任何时间依赖性的同时，在不同的时间步之间尽可能多地共享这个信息。
相比在每个位置$t$使用不同的条件概率分布，这个策略已经好很多了，但网络将必须在面对新$t$时进行推断更新。

为了更全面的吧RNN用图模型来解释，我们必须描述如何从这个模型中进行采样。
我们要做是每个不同的时间步的条件分布中进行采样。
然而，这儿有一个小麻烦。RNN必须有某种机制来确定序列的长度。当然，这可以通过很多方式来实现。

在当输出是属于某个词汇表中的符号时，我们可以在词汇表中添加一个特殊符号来表示序列终止(Schmidhuber, 2012)。
当这个符号出现时，采样过程停止。
在训练集中，我们将该符号作为一个额外成员插入序列的末尾，即紧跟每个训练样本$\Vx^{(\tau)}$之后。

另一种方法是在模型中引入一个额外的Bernoulli输出，表示在每个时间步之后决定继续生成还是终止。
这种方法比向词汇表增加一个额外符号的方法使用的更广泛，因为它可以适用于任何RNN模型，而不仅仅用于输出符号序列的RNN模型。
例如，它可以应用于一个产生实数序列的RNN模型。
新的输出单元通常使用sigmoid单元，损失函数则为交叉熵。
在这种方法中，sigmoid被训练为最大化正确预测的对数概率，即在每个时间步决定序列是否结束\footnote{译者注：此处不是很懂}。

确定序列长度$\tau$的另一种方法是将一个额外的输出添加到模型用来预测$\tau$本身。
模型可以采样出一个$\tau$的值，然后采样$\tau$步有价值的数据。
这种方法需要在每个时间步的循环更新的时候增加一个额外输入，使得循环在更新的时候知道它是否是靠近所产生序列的末尾\footnote{译者注：更新应该指的是产生条件概率}。
这种额外的输入可以是$\tau$的值，也可以是$\tau - t$，即剩下时间步的数量。
如果没有这个额外的输入，RNN可能会突然结束序列的产生，例如在产生句子时，可能句子还没有完整，生成就结束了。
这个方法可以用下面这个式子来解释：
\begin{align}
 P(\Vx^{(1)},\dots, \Vx^{(\tau)}) = P(\tau) P(\Vx^{(1)},\dots,\Vx^{(\tau)} \mid \tau) .
\end{align}
直接预测$\tau$的例子请参考 Goodfellow et al.(2014d)。

\subsection{通过RNN基于上下文对序列进行建模}
在上一小节，我们描述了如何从概率图模型的角度来看待RNN，还举了一个例子，在这个例子里没有输入$\Vx$，直接对随机变量序列$y^{(t)}$进行建模。当然，传统的RNN如等式\ref{eq:108a}所示，包含一个输入序列$\Vx^{(1)},\Vx^{(2)},\dots,\Vx^{(\tau)}$。一般情况下，我们在用图模型解释RNN的时候，不仅可以表示$y$变量的联合分布也能表示给定$\Vx$条件下，$y$条件分布。正如我们在\ref{sec:6.2.1.1}中介绍的前向网络一样，任何可以表示为$P(\Vy;\Vtheta)$的模型都能被重新写成条件分布$P(\Vy \mid \Vomega)$的形式，其中$\Vomega=\Vtheta$。
同样的，我们可以通过使用$P(\Vy \mid \Vomega)$代表分布$P(\Vy \mid \Vx)$来扩展这样的模型，但要令$\Vomega$是关于$\Vx$的函数。在RNN中，这个条件可以用多种不同的方式来实现。此处我们回顾一下最常用最有效的一些方法。

之前，我们已经讨论了将的向量$\Vx^{(t)}$序列作为输入传递给RNN，其中$t =1, \dots, \tau$。RNN也可以接受单个向量$\Vx$作为输入。
当$\Vx$是一个固定大小的向量，我们可以简单地将其看作产生$\Vy$序列RNN的一个额外输入。
将额外输入传递给RNN的一些常见方法是：
\begin{enumerate}
 \item 在每个时刻作为一个额外输入，或
 \item 把$\Vx$作为初始状态$\Vh^{(0)}$，或
 \item 两种方式同时使用。
\end{enumerate}

第一个也是最常用的方法在图\ref{fig:10_9}中进行了说明。
输入$\Vx$和每个隐藏单元向量$\Vh^{(t)}$之间的联系通过新引入的权重矩阵$\MR$来进行参数化的，这个权重矩阵在只包含$y$序列的模型中是没有的。
同样的，$\Vx^\top\MR$在每个时间步会作为隐藏单元的一个额外输入。
我们可以认为在确定$\Vx^\top\MR$值的时候，$\Vx$的选择可以看作是每个隐藏单元的一个偏置参数。
权重与输入保持独立。
我们可以把这种模型看作是由参数$\Vtheta$决定的非条件模型，我们可以把$\Vtheta$改成$\Vomega$，这时$\Vomega$里面的偏置参数是输入的函数。
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_9.PNG} 
   \caption{ 在这个图中，RNN将一个固定长度的向量$\Vx$映射为一个序列$Y$的分布。在做图像说明（image captioning）任务的时候我们常常使用这类模型。此时，图像就作为模型的输入，模型需要输出一个单词序列来描述这个图片。每个已观察到的输出序列里的元素$\Vy^{(t)}$会作为输入（给当前的时间步），同时它也是目标值（针对的是之前时间步）。}
   \label{fig:10_9}
\end{figure}

除了接收单个的一个向量作为输入以外，RNN也可以接收向量序列$\Vx^{(t)}$作为输入。等式\ref{eq:108a}描述的RNN对应的条件分布是$P(\Vy^{(1)}, \dots, \Vy^{(\tau)} ~|~ \Vx^{(1)}, \dots, \Vx^{(\tau)})$，根据条件独立性假设\footnote{译者注:在给定历史的输入$\Vy^{(t)} \mid \Vx^{(1)}, \dots, \Vx^{(t)}$的条件之后，$\Vy^{(t)}$直接相互条件独立}这个分布可以分解为
\begin{align}
 \prod_t P(\Vy^{(t)} \mid \Vx^{(1)}, \dots, \Vx^{(t)}).
\end{align}
为消除条件独立的假设，我们可以给在时刻$t$的输出到时间$t+1$的隐藏单元之间添加连接，如图\ref{fig:10_10}所示。
该模型就可以表示$\Vy$序列的任意概率分布。
这种给定一个序列的条件下表示另一个序列分布的模型的还是有一个限制，就是这给定序列和要表示的序列长度必须相等。在\ref{sec:10.4}我们介绍了如何取消这个限制条件。
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_10.PNG} 
   \caption{ 这是一个条件循环神经网络（conditional recurrent neural network），长度不定的输入序列$\Vx$映射成为一个和输入具有相同长度的序列$\Vy$的分布。和图\ref{fig:10_3}相比，这个模型中多了输出到下一状态节点之间的联系。这些联系可以帮助RNN给定序列$\Vx$的条件下，对$\Vy$的任意分布进行建模。而图\ref{fig:10_3}对应的模型在对$\Vy$进行建模时，必须保证给定$\Vx$的条件下，$\Vy$条件独立。}
   \label{fig:10_10}
\end{figure}
\section{双向RNN（Bidirectional RNNs）}
\label{sec:10.3}
目前为止我们考虑的所有RNN模型的例子都是“因果”结构的，意味着在时刻$t$的状态只能从历史序列$\Vx^{(1)},\dots,\Vx^{(t-1)}$以及当前的输入$\Vx^{(t)}$中获得信息。
在一些例子中，我们还讨论了在训练过程中，使用过去的目标值$\Vy$来影响当前状态。

但是，在许多应用中，我们要输出的预测$\Vy^{(t)}$可能依赖于整个输入序列。举个例子，在语音识别中，由于协同发音（co-articulation），语音会被分解成音素（phoneme），当前的语音所对应的音素的可能取决于未来几个音素，甚至由于相邻的单词之间存在语义依赖，该音素可能取决于未来的几个单词：如果当前的词有两种声学上合理的解释，我们可能要在更远的未来（和过去）来寻找信息从两个中选择一个正确的。
这在手写识别和许多其他序列到序列的学习任务中也是如此，将会在下一节对这类问题进行描述。

发明双向递归神经网络（bidirectional RNNs）的发明就是为了解决上一段提到的那些问题(Schuster and Paliwal, 1997)。当需要处理的任务有这种需求时，双向循环神经网络获得了非常大的成功 (Graves, 2012)，比如说手写识别 (Graves et al., 2008; Graves and Schmidhuber, 2009), 语音识别 (Graves and Schmidhuber, 2005; Graves et al., 2013) 以及生物信息学 (Baldiet al., 1999)。

顾名思义，双向循环神经网络由两个循环神经网络组成，一个是前向（forward）循环神经网络：从序列的开始向后处理数据。另一个事后向（backward）循环神经网络：从序列的的末尾向前处理数据。
图\ref{fig:10_11}展示了典型的双向循环神经网络的结构，其中$\Vh^{(t)}$是前向循环神经网络的状态节点，$\Vg^{(t)}$是后向循环神经网络的状态节点。
这时输出单元$\Vo^{(t)}$就可以同时考虑过去和未来的信息。当然$\Vo^{(t)}$的值对时刻$t$的输入值是很敏感的。此时我们不必像前馈神经网络，卷积神经网络或者是一个单向的具有固定大小的先行缓存器（ look-ahead buﬀer）的RNN一样，在$t$周围指定一个固定大小的窗口。模型可以获得这个窗口里面输入的信息。

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=6in]{fig/chap10/10_11.PNG} 
   \caption{一个典型的双向循环神经网络计算图，学习任务是将输入序列$\Vx$映射到目标序列$\Vy$，每一步的损失为$L^{(t)}$，$\Vh$的信息循环向前传播（从左往右），$\Vg$的信息循环向后传播（从右往左）。所以$\Vh^{(t)}$中包含了过去的输入信息而$\Vg^{(t)}$中包含了未来的输入信息，输出单元$\Vo^{(t)}$会同时参考$\Vh^{(t)}$和$\Vg^{(t)}$，那么也就可以同时考虑过去和未来的输入信息}
   \label{fig:10_11}
\end{figure}

这个想法可以扩展到2维的输入数据，比如说图像，这时我们就需要\emph{四个}RNN，沿着四个方向（上、下、左、右）来对数据进行处理。
如果循环神经网络能够学习到更多的上下文信息，那在计算2维网格上的每个点$(i, j)$的输出$O_{i,j}$就能可以不仅仅只考虑局部信息，还能够考虑距离位置$(i, j)$更远的信息。
相比于卷积神经网络，在图像上使用RNN通常更复杂，但RNN就允许特征图的特征之间存在长期横向的相互作用\footnote{译者注：卷积神经网络只可以考虑到局部的联系，即只能考虑到那个固定窗口里面的信息，而循环神经网络则可以获得更广的上下文信息，不过四向RNN应用的很少}(Visin et al., 2015; Kalchbrenner et al., 2015)。
实际上，对于这样的循环神经网络模型，前向传播公式可以写成卷积的形式，在包含横向联系的特征图上进行递归计算之前，在每一层自底向上对输入进行处理\footnote{译者注：原文表达不清晰，不知道怎么翻译}。

\section{基于编码-解码的序列到序列架构}
\label{sec:10.4}
我们已经在图\ref{fig:10_5}看到RNN如何将输入序列映射成固定大小的向量，在图\ref{fig:10_9}中展示了RNN如何将固定大小的向量映射成为一个序列，在图\ref{fig:10_3}、\ref{fig:10_4}、\ref{fig:10_10}和\ref{fig:10_11}中看到RNN如何将一个输入序列映射到与输入等长的输出序列。

本节我们讨论如何训练RNN，使其将输入序列映射到任意长度的输出序列。
这在许多机器学习任务中都有使用，如语音识别、机器翻译或问答系统，其中训练集的输入和输出序列的长度通常不相同（虽然它们的长度可能有联系）。

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{fig/chap10/10_12.PNG} 
   \caption{基于编码-解码的序列到序列架构的一个实例。根据输入序列$(\Vx^{(1)},\Vx^{(2)} \dots, \Vx^{(n_x)})$产生输出序列$(\Vy^{(1)},\Vy^{(2)} \dots, \Vy^{(n_y)})$。这个模型有两部分RNN组成，第一个编码RNN用来读入序列，第二个解码RNN用来产生输出序列（或者说计算输出序列的分布）。编码RNN的最后一个隐藏状态可以用固定大小的变量$C$（contex）来表示，$C$里面存储这对输入序列语义的总结信息。$C$也会作为输入传递给解码RNN用来生成输出序列。}
   \label{fig:10_12}
\end{figure}

我们通常较RNN的输入为\textbf{上下文}（contex），我们通常用$C$来表示上下文信息。
这个上下文$C$可能是对输入序列$\MX=(\Vx^{(1)},\dots,\Vx^{(n_x)})$信息的概括。他可以是向量或者向量序列。

用于将长度不定的序列映射到另一一个长度不定序列的RNN架构最初由 Cho et al. (2014a)提出，不久之后 Sutskever et al. (2014)对这个架构进行了发展，并把这种方法应用到机器翻译翻译中，取得了较好结果。这个系统是对另一个机器翻译系统产生的翻译结果进行评分，而后者使用一个传统的递归神经网络来生成翻译结果。
这些作者对这个架构\ref{fig:10_12}不同的称呼，通常叫编码-解码或序列到序列架构。
这个架构的本质非常简单：（1）用\textbf{编码}或\textbf{读取器}(reader)或\textbf{输入}(input)RNN接收输入序列。
这个编码RNN最后会输出上下文$C$（通常是对最后一个隐藏状态进行处理输出）。
(2)\textbf{解码}或\textbf{写入器}(writer)或\textbf{输出}(output)RNN则接收固定长度的向量$C$（如图\ref{fig:10_9}）然后生成输出序列$\MY=(\Vy^{(1)}, \dots, \Vy^{(n_y)})$。
这种架构的创新之处在于输入输出序列的长度$n_x$和$n_y$可以彼此不同，而前面介绍的RNN的架构基本都要求输入序列长度等于输出序列长度$n_x = n_y = \tau$。
在这个序列到序列的架构中，我们需要同时训练这两个RNN来最大化训练集中所有$\Vx$和$\Vy$对数条件概率$\log P( \Vy^{(1)}, \dots, \Vy^{(n_y)} ~|~ \Vx^{(1)},\dots,\Vx^{(n_x)} )$。
编码RNN的最后一个隐藏状态$\Vh_{n_x}$通常被当作输入的上下文$C$，或者是输入序列信息的总结，并作为解码RNN的输入。

如果上下文$C$是一个向量，则解码RNN的结构和我们在\ref{sec:10.2.4}节介绍的一样，是一个将向量映射到序列RNN结构。
正如之前介绍的那样，将向量映射到序列的RNN至少有两种方式来接受输入向量。
输入可以作为RNN的初始状态，或也可以作为输入传给每一个时间步中隐藏单元。这两种方式也可以结合起来使用。

这个结构并不限制编码RNN和解码RNN具有相同大小的隐藏层。

但是这个架构有一个明显的不足：如果编码RNN输出的上下文$C$的维度太小，它不能将输入序列的有用的信息全部存进这个向量$C$。
这是由 Bahdanau et al. (2015)在用这个架构做机器翻译的时候观察到的。
他们提出让$C$成为长度可变的序列，而不是一个固定大小的向量。
此外，他们还介绍了注意力机制（attention mechanism），将序列$C$的元素和输出序列的元素相关联起来。更多的细节会在\ref{sec:12.4.5.1}进行介绍。

\section{深度RNN结构}
\label{sec:10.5}
大多数RNN的计算可以分为三个模块，当然也对应着三组参数和三个变换映射：
\begin{enumerate}
 \item 从输入到隐藏状态，
 \item 从前一隐藏状态到下一隐藏状态，
 \item 从隐藏状态到输出。
\end{enumerate}
\ref{fig:10_3}是传统的RNN的结构，这三个模块每个都有一个与之相关的权重矩阵。或者说，当我们把RNN展开时，每个模块对应一个简单的变换，而这个变化是可以用单层或者多层的MLP来表示的。通常这个转换分为两步，显示一个仿射变换，紧接着把仿射变换的结果进行非线性变换\footnote{sigmoid，tanh}。

那么如果我们使用更复杂的变换，或者原来只有一层的MLP，我们把它变成多层，这样会不会让我们的模型有一个更好的表现呢？
 (Graves et al., 2013; Pascanu et al., 2014a)里的实验证明了这一点。我们需要足够深的网络结构来获得理想的变换映射。 一些早期深度RNN的介绍可以参考 Schmidhuber (1992),El Hihi and Bengio (1996), 或者 Jaeger (2007a)。
 
 Graves et al.(2013) 首先发现了像\ref{fig:10_13}（左）一样，将RNN的状态分为多层可以给模型的表现带来显著的提升，如。
我们可以认为，在\ref{fig:10_13}(a)所示在多层结构中最开始的那些层主要将原始输入进行抽象转换，输出一个对隐藏状态更合适，更高层次的表示。
Pascanu et al. (2014a)进一步提出在上述三个模块中分布使用一个单独的MLP结构（可以是多层的），如\ref{fig:10_13}(b)所示。
考虑到模型的表示能力，我们建议在这三个模块合理分配足够的容量，但当我们增加模型的深度可能会造成优化困难而损害学习效果。
在一般情况下，浅层的架构更容易优化，如果我们像\ref{fig:10_13}(b)加入额外的层会导致从时间点$t$的变量到时间点$t+1$的最短路径变长。
例如，如果单隐藏层的MLP被用于状态到状态的转换，那么我们在\ref{fig:10_13}使用的结构任何两个不同时间点变量之间最短路径的长度会加倍。
然而Pascanu et al（2014a）认为，在隐藏状态到隐藏状态的路径中引入跨越连接（skip connection）如\ref{fig:10_13}(c)所示，可以缓和这个问题。

 \begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{fig/chap10/10_13.PNG} 
   \caption{一个循环神经网络可以通过不同的方式来加深网络的层数(Pascanu et al, 2014a)。（a）RNN中的隐藏状态可以被分成多组层次结构。（b）在输入到隐藏状态，隐藏到隐藏状态和隐藏到输入状态中加深结构（比如说吧MLP由一层转为多层）。这会延迟不同时间步节点之间的最短路径的长度。（c）路径延长的副作用可以通过跨越连接来缓和。}
   \label{fig:10_13}
\end{figure}


\section{递归神经网络（Recursive Neural Networks）}
\label{sec:10.6}
递归神经网络\footnote{我们建议不要将递归神经网络缩写为RNN，以免与循环神经网络发生混淆。}是循环神经网络的另一个扩展，它计算图为树状结构而不是传统循环神经网络的链状结构。
递归网络的典型的计算图结构如\ref{fig:10_14}所示。
递归神经网络由Pollack (1990)引入，而Bottou (2011)描述了递归神经网络的潜在用途——学习推论。
自然语言处理(Socher et al., 2011a,c, 2013a)领域和计算机视觉(Socher et al., 2011b)领域，递归网络已成功地应用于输入为\emph{数据结构}的神经网络(Frasconi et al., 1997,1998)。

递归网络的一个明显优势是，对于长度为$\tau$的序列，深度（通过非线性操作的组合数量来衡量）可以急剧地从$\tau$减小为$\CalO(\log \tau)$，这可能有助于解决下一小节提到的长时依赖（long-term dependencies）问题。
一个还没有解决的问题是如何以最佳的方式构造树。一种选择是使用不依赖于数据的树结构，如平衡二叉树。
在某些应用领域，外部方法可以帮助我们选择适当的树结构。
例如，处理自然语言中的句子时，可以用自然语言语法分析器对句子构造一个语法分析树，作为递归网络的树结构 (Socher et al., 2011a, 2013a)。
理想的情况下，人们希望学习器根据任意输入自行发现和推断合适的树结构，如 Bottou (2011)所建议的那样。
 
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{fig/chap10/10_14.PNG} 
   \caption{递归神经网络的计算图有传统循环神经网络的链式结构变成了树状结构。长度不定的序列$\Vx^{(1)},\Vx^{(2)},\dots,\Vx^{(t)}$可以通过固定的参数集合（权重矩阵$\MU$、$\MV$、$\MW$）被映射为一个固定大小的表示（输出向量$\Vo$）。上图展示了一个监督学习实例，每个序列对应着一个目标$\Vy$。}
   \label{fig:10_14}
\end{figure}
 
有很多对递归神经网络的变种。举个例子，Frasconiet al. (1997) 和 Frasconi et al. (1998)把数据和一个树结构联系起来，输入和目标值都是树结构中的单独的节点。每个树节点进行的计算和传统的人工神经网络的计算方式不一样（通常是将输入向量进行仿射变换，然后将变换后的结果通过一个单调的非线性单元。）例如, Socher et al. (2013a)提出了使用张量运算和双线性形式来对概念（concept）之间的联系进行建模。 (Weston et al., 2010; Bordes et al., 2012)已经证明了当概念是有一个里面元素为连续值得向量来表示（embeddings）的时候，张量运算和双线性模式对概念间的联系建模很有用。


\section{长时依赖（Long-Term Dependencies）的挑战}
\label{sec:10.7}
递归神经网络在虚席过程中会遇到长时依赖的问题，这个问题的细节我们在\ref{sec:8.2.5}进行了介绍。产生这个问题的根本原因是梯度经过多步传播之后可能会产生弥散（大部分情况）或者爆炸（很少，但是会对优化过程产生不好的影响），尽管我们假定我们设置参数来使RNN保持（拥有记忆功能，可以存储历史信息，梯度不会爆炸），但是当参数的值很小，而序列的长度很长而且相关联的信息之间的距离也很长，这个时候就很容易产生长时依赖问题\footnote{译者注：这儿没有例子，读者理解起来可能会比较困难，建议加个例子。}。 (Hochreiter, 1991; Doya, 1993; Bengio et al., 1994：Pascam et al，2013）对这个问题进行了更深层次的讨论。在这一小结，我们对这个问题进行了仔细的描述。在本章剩余的章节中我们介绍一些方法来解决这个问题。

循环神经网络会在每一个时间步使用相同的函数。这些函数多次重复的组合之后就会造成极端的非线性现象\footnote{ 译者注：这部分用中文表达略有困难，主要的意思就是对数据连续进行多次非线性函数映射，可能函数的形式是一样的，参数不同，会产生比较奇怪的非线性现象。}。如图\ref{fig:10_15}所示。

\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{fig/chap10/10_15.PNG} 
   \caption{当我们把多个非线性函数（想图中展示的tanh层）组合到一起，会产生较强的非线性。尤其是当其中的大部分值得倒数比较小，有一部分值得倒数比较大，而且增加减小交替变换非线性会变得更严重。在这幅图中，我们将一个100维的隐藏状态投影（主要为了降维）到1维空间并在$y$坐标中展示，$x$坐标轴表示的是初始100维的隐藏状态随机的一个维度方向。我们可以把这曲线看作是高维函数的一个线性的横截面。不同的曲线代表不同的时间步，每个事件不得转换函数都会把之前的那些转换函数累积起来。}
   \label{fig:10_15}
\end{figure}

我们循环神经网络中进行的函数组合有点像连续的矩阵乘法，我们考虑一个简化版的循环神经网络，递推关系
\begin{align}
 \Vh^{(t)} = \MW^\top \Vh^{(t-1)}
\end{align}
里面没有非线性激活函数和输入$\Vx$。如在\ref{sec:8.2.5}中描述的那样，递推关系的本质就是矩阵的幂乘。我们可以把它简化为:
\begin{align}
 \Vh^{(t)} = (\MW^t)^\top \Vh^{(0)},
\end{align}
当$\MW$可以特征分解成下面的形式
\begin{align}
 \MW = \MQ \VLambda \MQ^\top,
\end{align}
其中$\MQ$为正交矩阵，递推关系式也可以进一步简化为
\begin{align}
 \Vh^{(t)} = \MQ^\top \VLambda^t \MQ \Vh^{(0)}.
\end{align}
特征值进行$t$次幂乘之后，幅值不到一的特征值会衰减到零，而幅值大于一的就会激增。
分解之后，$\Vh^{(0)}$中任何不被投影到最大的特征值对应的那个维度上的部分将最终将被丢弃\footnote{译者注：结合奇异值分解，主成分分析法来理解这句话}。

这个问题主要在循环神经网络中比较严重。在标量的情况下，考虑我们对权重$w$做多次幂乘，$w^t$要么消失（$w$的绝对值小于1）要么激增（$w$的绝对值大于1）。但是如果我们考虑一个非循环神经网络，在每个时间步使用不同权重$w^{(t)}$，那么情况就不同了。
如果初始状态为$1$，那么时刻$t$的状态可以由$\prod_t w^{(t)}$给出。
假设$w^{(t)}$的值是随机生成的，各自独立，均值为$0$，方差为$v$。
$\prod_t w^{(t)}$的方差就为$\CalO(v^n)$。
为了获得某些我们所期望的方差$v^*$，我们可以分别选择不同的权重，并让那些权重的方差为$v=\sqrt[n]{v^*}$。

非常深的前馈神经网络可以通过精心设计权重来避免梯度消失和梯度爆炸。在Sussillo (2014）中做了介绍。

梯度消失和梯度爆炸问题是游不同的研究者 (Hochreiter, 1991; Bengio et al., 1993, 1994)分别发现的。有的人可能希望如果我们只在某个固定的区域空间选择权重来避免梯度消失和梯度爆炸。不幸的是为了储存历史信息，并能够承受小扰动，RNN在训练过程中必须让参数进入会造成梯度消失的一些区域 (Bengio et al., 1993,1994)。尤其是我们希望模型能够表示长时依赖的时候，长时交互的梯度和短视交互的梯度相比会呈指数级减小。当然这不是说我们不能进行训练学习了，只是我们需要跟多的时间来学习长时依赖，因为长时依赖关系的信息会被短期依赖信息的一些小的波动来取代掉。实际上， Bengio et al. (1994)的实验表明，当我们增加了需要获得的依赖关系信息的跨度，基于梯度的优化会变得越来越困难，当我们在在长度仅为10或20的序列上用SGD对RNN进行训练时，成功的概率迅速变为0。

如果想从动态系统的观点来看待RNN，可以阅读一下 Doya(1993), Bengio et al. (1994) ， Siegelmann and Sontag (1995), 和一篇综述： Pascanu et al. (2013).
本章的其余部分将讨论目前已经提出的缓和长时依赖问题的方法（在某些情况下，允许一个RNN模型学习横跨数百步的依赖关系），但长时依赖问题仍是深度学习中的一个主要挑战。

\section{回声状态网络（Echo State Networks）}
\label{sec:10.8}
在循环神经网络中，将$\Vh^{(t-1)}$映射到$\Vh^{(t)}$的权重矩阵和映将$\Vx^{(t)}$映射到$\Vh^{(t)}$的权重矩阵是最难进行优化学习的。

Jaeger, 2003; Maass et al., 2002; Jaeger and Haas, 2004;Jaeger, 2007b提出避免这种困难的方法是：设置合适的递归权重使得隐藏单元可以很好地记录历史输入信息，这样我们\emph{只需要学习输出权重}。这个想法分别被用于回声状态网络（echo state networks 或者 ESN） (Jaeger and Haas, 2004; Jaeger, 2007b) 和液态机 （liquid state machines）(Maass et al., 2002). 这两个结构比较像，只不过液态机使用的脉冲神经元（输出为二进制）而回声状态网络的隐藏单元的值是连续的。回声状态网络和液态机都被称作容器计算结构（reservoir computing）(Lukoševičius and Jaeger, 2009)。因为他们连个结构的隐藏单元变成了存储时许特征的容器，时序特征是不同方面的历史输入信息。

这类容器计算结构的神经网络可以看作是核机器（kernel machines）\footnote{回想一下支持向量机}：他们将一个任意长度的序列（到时刻$t$的历史输入）映射成为一个固定长度的向量（循环状态$\Vh^{(t)}$），之后将向量通过一个线性预测器（通常是线性回归）来解决我们的问题。
训练的损失函数可以很容易地设计为输出权重的凸函数。
例如，如果输出是隐藏单元到输出目标的一个线性回归问题，训练的损失函数就可以是均方误差。这是一个凸函数，一样我们可以很容易的找到最优解 (Jaeger, 2003)。

现在我们所面临的问题是：如何设置我们的输入和递归权重使我们循环神经网络的隐藏状态可以存储序列的历史信息？这个答案在介绍容器计算的文章中已经给出了。我们可以把循环网络看作是一个动态系统。设置输入和循环的权重是这个动态系统接近稳定。

最初的想法是使神经网络中状态到状态转换函数的雅可比矩阵的特征值接近1。如在\ref{sec:8.2.5}中介绍的那样，循环神经网络的一个重要特征就是雅可比矩阵的特征值谱$\MJ^{(t)} = \frac{\partial s^{(t)}}{\partial s^{(t-1)}}$。其中$\MJ^{(t)}$的谱半径是特征值绝对值的最大值。

为了理解一下谱半径的影响，我们举一个简单的例子。假设我们在做后向传播算法的时候，雅可比矩阵$\MJ$不随$t$改变（如果网络状态的转移函数是纯线性的，那么可能出现这种情况）。

假设$\MJ$特征值$\lambda$对应的特征向量为$\Vv$。
考虑当我们把梯度向量后向传播时会发生什么。
如果最开始的梯度向量为$\Vg$，然后经过一步的后向传播之后，梯度向量变为$\MJ \Vg$，在$n$步之后，梯度向量为$\MJ^n \Vg$。
现在考虑一下我们在后向传播$\Vg$的时候加一个小的噪声发生什么。
如果最开始为$\Vg + \delta \Vv$，一步之后，我们会得到$\MJ(\Vg + \delta \Vv)$。
$n$步之后，我们将得到$\MJ^n(\Vg + \delta \Vv)$。
由此我们可以看出，由$\Vg+\delta \Vv$的结果比由$\Vg$开始的结果，$n$步之后偏离了$\delta \MJ^n \Vv$。
如果$\Vv$选择为$\MJ$特征值$\lambda$对应的一个单位特征向量，那么在每一步乘雅可比矩阵，只是对原梯度向量进行简单地缩放。
后向传播的两次执行偏离的距离为$\delta | \lambda |^n$。
当$\Vv$为绝对值$|\lambda|$最大的特征值对应的特征向量，噪声$\delta$会造成最大的偏离量。

当$ | \lambda | > 1$，偏差$\delta | \lambda |^n$就会呈指数级增长。
当$ | \lambda | < 1$，偏差就会变呈指数级减小。

当然，在这个例子中我们假设每一步的雅可比矩阵是不变的，这个情况只在线性循环神经网络中才会出现。
当网络中有非线性映射是，非线性函数的导数将在多步的后向传播之后后接近零，这有助于防止因过大的谱半径而造成梯度爆炸。
事实上，回声状态网络的最近工作提倡使用远大于1的谱半径 (Yildiz et al., 2012; Jaeger, 2012)。

我们之前提到过在后向传播的时候使用矩阵的幂乘，如果网络是线性的，这也可以被用到前向传播过程中。状态$\Vh^{(t+1)} = \Vh^{(t)\top} \MW $

当我们把$\Vh$经过一个线性映射$\MW^\top$之后，它的$L^2$范数总是缩小，那么我们说这个映射是收缩（contractive）的。
当谱半径小于一，则从$\Vh^{(t)}$到$\Vh^{(t+1)}$的映射是收缩的，因此小变化在每步之后后变得更小。
当我们使用有限精度（如32位整数）来存储状态向量时，必然会使得网络忘掉过去的信息。

雅可比矩阵告诉我们$\Vh^{(t)}$一个微小的搅动是如何一步步前向传播的，以及，$\Vh^{(t+1)}$的梯度如何一步步后向传播的。
需要注意的是，$\MW$和$\MJ$都不需要是对称矩阵（尽管它们是实方阵），因此它们的特征值和特征向量里的元素可以是负数，其中虚数分量对应于潜在的振荡行为（如果每一步的雅可比矩阵是一样的话）。
即使$\Vh^{(t)}$或$\Vh^{(t)}$上的一个搅动都是实数。在后向传播之后可能会造成复数的偏差。
我们所关注的是，当我们用这些矩阵来乘向量，这些复数偏差的幅值（复数的模值）会发生什么变化。
如果特征值的幅值大于一，那么回造成梯度放大（如果我们连续的乘以这个矩阵，会造成指数级增长）。或收缩（如果我们连续的乘以这个矩阵，会造成指数级减小）\footnote{译者注：此处存疑，未讨论幅值小于一的情况，而且是不是幅值大于1才会造成振荡？}。

如果系统里面有非线性的映射函数，那么雅可比矩阵在每个时间步都有可能变化。这个动态系统会变得更复杂。但是，初始值的一个小的搅动经几部之后依然会被放大为一个大的偏差。和线性的动态系统不同的是，如果我们的非线性单元是类似tanh那种压缩非线性单元\footnote{译者注：任何值都会被映射到一个有界的区域。}，那么这个循环动态系统状态的值会有界。注意，即使前向传播由于压缩非线性单元会变得有界，在做后向传播时，是有可能没有这个界的。例如，在系统中那个非线性单元为$\tanh$，序列中的元素传入这个非线性单元之前会由谱半径大于1的权重矩阵将之映射为一个值。
然而，所有$\tanh$单元同时位于它们的线性激活点是非常罕见的\footnote{译者注：此处不理解}。

回声状态网络的策略是固定权重为拥有类似于3的谱半径的矩阵，信息通过时间前向传播，但会由于容易饱和的非线性单元（如$\tanh$）的稳定作用而不会产生信息爆炸。

最近的研究表明，这些设置回声状态网络权重的技术可以用于循环神经网络的初始化（如果这个网络的隐藏到隐藏状态的权重可以通过后向传播算法来进行训练的话），可以帮助我们学习长时依赖信息 (Sutskever, 2012; Sutskeveret al., 2013)。在设置的时候，如果初始化权重的谱半径是1.2可以获得一个好的效果。同时我们还可以采取\ref{sec:8.4}节介绍的稀疏初始化的策略。

\section{Leaky单元和其他处理多个时间尺度的策略}
\label{sec:10.9}
一种解决长时依赖问题的方法是设计一个模型用来处理不同的时间尺度的依赖问题。所以模型的一部分用来处理细微的时间尺度并能处理一些小的细节。模型的其他部分可以处理大的时间尺度并能高效地把遥远的历史信息传递给当前的状态。可以用不同的策略来实现这两个功能。我们可以在时间轴上增加跳跃连接（skip connection），Leaky单元可以整合某些固定时间点的信号，去除一些用于细微时间尺度的建模的连接。

\subsection{时间轴上建立跳跃连接}
\label{sec:10.9.1}
一种处理大的时间尺度上依赖的方法是直接在过去的某个变量和当前变量之间建立直接联系。这个建立跳跃连接的想法醉在是由 Lin et al. (1996)提出的，后来在 (Lang and Hinton, 1988)中，在前馈神经网络中引入了单位延迟单元。这样就产生了循环神经网络，从时刻$t$的单元连接到时刻$t+1$单元构建一个循环连接。我们也可以用一个更长的延迟单元来建立循环神经网络 (Bengio, 1991)。

我们在\ref{sec:8.2.5}中看到，随着序列长度的增加，梯度会发生消失或者爆炸现象。 Lin et al. (1996)中介绍通过引入$d$时延的循环连接来缓解这个问题。
现在导数指数减小的速度是$\frac{\tau}{d}$而不是$\tau$的函数。
既然同时存在延迟和单步连接，梯度仍可能呈$\tau$指数爆炸。
这时学习算法获得一些更长的依赖信息，但不是所有的长时依赖信息可以通过这种方式完美的获得。

\subsection{Leaky单元和和一系列不同时间尺度}
\label{sec:10.9.2}
使某条路径上导数乘积接近1的另一方式是设置\emph{线性}自连接单元，并且这些连接的权重值接近1。

当我们用增量计算的方法\footnote{译者注：可以了解一下增量学习，在线学习}来计算$v^{(t)}$的均值$\mu^{(t)}$，更新公式为$\mu^{(t)} \gets \alpha \mu^{(t-1)} + (1-\alpha) v^{(t)}$，其中$\alpha$是一个从$ \mu^{(t-1)}$到$ \mu^{(t)}$线性自连接的例子。 
当$\alpha$接近1时，这个动态的平均值能记住过去很长一段时间的信息，而当$\alpha$接近0，历史信息会被迅速丢弃。拥有线性自连接结构的隐藏单元和增量均值类似。这种隐藏单元我们叫它Leaky单元。

$d$时间步的跳跃连接可以确保当前的单元总能被之前$d$个时间步之前的值所影响。
使用权重接近1的线性自连接是另一种方法确保该单元可以获得过去值得信息。
和对跳跃连接调整步长的方式相比，线性自连接可以通过调节$\alpha$来平滑灵活地调整影响。

这个想法是由Mozer (1992) and by El Hihi and Bengio (1996)提出的。在回声状态网络中，Leaky单元也被发现很有用(Jaeger et al., 2007)。

有两种基本策略来设置Leaky单元使用的时间常数。一种是手动的把他们的值设置为一个常数。例如在初始化时从某些分布进行采样一些值来设置它们。
另一种策略是使把时间常数作为变量，并学习出来。
在不同时间尺度使用Leaky单元似乎能帮助我们解决长时依赖问题 (Mozer, 1992; Pascanu et al., 2013)。


\subsection{连接移除}
\label{sec:10.9.3}
另一种解决长时依赖问题的方法是合理组织不同时间尺度上RNN的状态 (El Hihi and Bengio, 1996)。当时间尺度变慢的时候（把一些连接给去了），信息更容易流动。

这和我们前面提到的跨越连接不一样，因为它会主动\emph{删除}一些长度为1的连接，并用更长的连接来替换它们。我们通过这种方式来修改单元可以帮助我们操作一个更长的时间尺度。跳跃连接会在单元间增加一些边。这些新的连接所连接的单元又可以能会从新的长的时间尺度去获取信息，也有可能它们只专注于那些短的连接所传递过来的信息。

让一组循环单元处理不同时间尺度的数据有不同的方式。一个是使用Leaky单元，但是不同组的单元对应不同的固定的时间尺度。这个最初在 Mozer (1992)提出并在Pascanuet al. (2013)中成功进行了应用。

另一种方式是使显式离散地在不同的时间步进行更新，不同组的单元对应不同的更新频率。这个方法由El Hihi and Bengio (1996) and Koutnik et al. (2014)提出，并在一些数据集中获得了较好的效果。

\section{长短时记忆网络（LSTM）和其它有门结构的RNN（Gated RNN）}
\label{sec:10.10}
在纂写本书的时候，实际应用中最有效的序列模型是带门结构的RNN（Gated RNN）。它们包括长短时记忆网络和基于门随机单元（gated recurrent unit 或 GRU）的网络。

和Leaky单元一样，Gated RNN也是重新在时间轴上生成路径，来避免导数消失和导数爆炸。
在Leaky单元里，我们可以通过手动选择常数作为连接权重或把它作为一个需要学习的参数来达到这一目的。
Gated RNN将其发展为在每个时间步的连接权重都可能发生改变。

Leaky单元允许网络累积较长一段时间的信息（信息主要是指某些特定特征或类的信息）。但是，一旦这些信息被使用之后，我们有时可以把这些旧的状态的信息给丢掉。比如说，
如果一个序列是由子序列组成，我们希望Leaky单元能在每个子序列内部累积信息，我们需要建立一个机制来忘记的状态，通常我们将状态设置为0就表示忘记了该状态。
我们希望神经网络学会决定何时丢弃一些状态信息，而不是手动决定。
这就是Gated RNN要做的事。

\subsection{LSTM}
\label{sec:10.10.1}
在网络结构中引入自循环来产生一条供梯度长时间流动且不会消失的路径是最初长短时记忆网络（long short-term memory 或 LSTM）的核心思想 (Hochreiter and Schmidhuber, 1997)。一个重要的变化是自循环上的权重是有上下文决定的，而不是固定的 (Gers et al., 2000)。在这个自循环的的权重上加一个门限（gate）（这个门限是由另一个隐藏单元决定的），不同时间尺度累积的信息可以动态地改变。
在这种情况下，即使是具有固定参数的LSTM，不同时间尺度上累积的信息也可以根据输入序列来改变，因为时间常数是模型本身的输出。LSTM在很多应用用取得了重大的成功。比如说无约束手写识别(Graveset al., 2009)，语音识别(Graves et al., 2013; Graves and Jaitly, 2014)，手写文字生成(Graves, 2013), 机器翻译(Sutskever et al., 2014),图像说明(Kiros et al., 2014b; Vinyals et al., 2014b; Xu et al., 2015) 和语义分析 (Vinyals et al., 2014a)。
\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{fig/chap10/10_16.PNG} 
   \caption{这是LSTM循环神经网络的cell结构。Cell之间是循环联系的，LSTM就是把传统单隐藏层的循环神经网络的隐藏单元换成了Cell。输入特征是由一个普通的人工神经元单元计算得到的。如果输入门（input gate）允许，输入信息可以传递给cell的状态。cell的状态有一个线性自连接结构，其中的权重是有忘记门（forget gate）来控制的。cell的输出由输出门来控制。所有的门，都有一个sigmoid函数，输入门可以有任意的压缩非线性。状态单元的值也可以作为门单元的一个额外的输入。黑色的方块表示的是一个时间单位的时延}
   \label{fig:10_16}
\end{figure}

图\ref{fig:10_16}是LSTM的模块结构示意图。我们在下面给出了浅层循环神经网络的前向传播公式。在(Graveset al., 2013; Pascanu et al., 2014a)中成功的使用了更深的结构。和传统的循环神经网络不同，LSTM不仅仅是将循环单元\footnote{译者注：隐藏状态}和输入合并的向量进行仿射变换然后再进行一个非线性系统，它有一个LSTM cell结构，在LSTM cell里面有一个内部循环（自连接循环）。这个cell的输入输出和传统的循环神经网络一样，但是cell里面有更多的参数，同时还有门单元来控制信息的流动。这里面最重要的部分是cell状态单元$s_i^{(t)}$，它与前一节讨论的Leaky单元类似，有线性自环。
然而，此处自环的权重（或相关联的时间常数）由记忆门（forget gate）$f_i^{(t)}$控制（$t$为时间步，$i$为cell状态向量对应的下标），记忆门中的非线性sigmoid单元会将权重设置为0和1之间的值：
\begin{align}
 f_i^{(t)} = \sigma \Big( b_i^f + \sum_j U_{i,j}^f x_j^{(t)} + \sum_j W_{i,j}^f h_j^{(t-1)} \Big),
\end{align}
其中$\Vx^{(t)}$是当前输入向量，$\Vh^{t}$是当前隐藏层向量，$\Vh^{t}$包含所有LSTM cell的输出信息。 
$\Vb^f, \MU^f, \MW^f$分别是偏置参数、输入权重和记忆门的循环权重。
因此LSTM cell状态以如下方式更新，其中有一个由记忆门产生的自环权重$f_i^{(t)}$来控制历史信息：
\begin{align}
 s_i^{(t)} = f_i^{(t)}  s_i^{(t-1)} +  g_i^{(t)}
 \sigma \Big( b_i + \sum_j U_{i,j} x_j^{(t)} + \sum_j W_{i,j} h_j^{(t-1)} \Big),
\end{align}
其中$\Vb, \MU, \MW$分别是LSTM cell中的偏置、输入权重和记忆门的循环权重。
\textbf{外部输入门}(external input gate)单元$g_i^{(t)}$和记忆门（使用一个sigmoid函数来获得0和1之间的值）的更新方式类似，但只不过使用了不同的参数：
\begin{align}
 g_i^{(t)} = \sigma \Big( b_i^g + \sum_j U_{i,j}^g x_j^{(t)} + \sum_j W_{i,j}^g h_j^{(t-1)} \Big).
\end{align}
LSTM cell的输出$h_i^{(t)}$也可以由\textbf{输出门}(output gate) $q_i^{(t)}$来控制，它也使用sigmoid单元作为门限：
\begin{align}
 h_i^{(t)} &= \text{tanh}\big( s_i^{(t)} \big) q_i^{(t)}, \\
 q_i^{(t)} &= \sigma \Big( b_i^o + \sum_j U_{i,j}^o x_j^{(t)} + \sum_j W_{i,j}^o h_j^{(t-1)} \Big),
\end{align}
其中$\Vb^o, \MU^o, \MW^o$分别是其偏置、输入权重和记忆门的循环权重。
在LSTM的一些变体中，可以选择使用cell状态$s_i^{(t)}$作为额外的输入，输入到第$i$个单元的三个门\footnote{译者注：之前gate的输入为当前输入，上一个输出，这个时候就变成当前输入，上一个输出，上一个cell状态}，如\ref{fig:10_16}所示。
这是我们将需要三个额外的参数\footnote{译者注：因为有三个门}。

LSTM神经网络比传统的神经网络更容易获得长时依赖信息。它也被成功应用于不同的任务。首先它被用于在人工设计的数据集中测试能否缓解长时依赖问题(Bengio et al., 1994; Hochreiterand Schmidhuber, 1997; Hochreiter et al., 2001)。接着人们用它来进行序列处理任务是获得了较好的结果(Graves, 2012;Graves et al., 2013; Sutskever et al., 2014)。 LSTM的其它一些变种模型我们在下一小节进行讨论。


\subsection{其它带门结构的RNN}
\label{sec:10.10.2}
LSTM中哪些结构是真正所必须的？还有哪些其他的结构可以允许网络动态的的控制时间尺度和不同单元的遗忘行为？

最近研究的带门结构的RNN给出了这些问题的一些答案。在这些结构中它们包含了门限循环单元（gated recurrent units 或 GRUs） (Cho et al., 2014b;Chung et al., 2014, 2015a; Jozefowicz et al., 2015; Chrupala et al., 2015)。它和LSTM最大的不同是：它只用一个门就同时控制状态单元决定忘记哪些信息，更新哪些信息。更新等式如下：
\begin{align}
 h_i^{(t)} = u_i^{(t-1)} h_i^{(t-1)} + (1 - u_i^{(t-1)}) \sigma 
 \Big( b_i + \sum_j U_{i,j} x_j^{(t)} + \sum_j W_{i,j} r_j^{(t-1)} h_j^{(t-1)} \Big),
\end{align}
其中$\Vu$代表更新门，$\Vr$代表复位门。
它们的值定义如下：
\begin{align}
 u_i^{(t)} = \sigma \Big( b_i^u + \sum_j U_{i,j}^u x_j^{(t)} + \sum_j W_{i,j}^u h_j^{(t)} \Big),
\end{align}
和
\begin{align}
 r_i^{(t)} = \sigma \Big( b_i^r + \sum_j U_{i,j}^r x_j^{(t)} + \sum_j W_{i,j}^r h_j^{(t)} \Big).
\end{align}
复位和更新门能分别地在每个时间步"忽略"状态向量的一部分。
更新门像条件Leaky累积器一样，可以给任意的维度加一个线性门限，从而选择将它完全保存下来（sigmoid值取0）或完全由新的"目标状态"值（朝Leaky累积器的收敛方向）替换并完全忽略它（在另一个极端，sigmoid值取1）。
复位门控制状态的哪一部分用于计算下一个目标状态，在过去状态和未来状态的关系之间引入了新的非线性效应。

我们可以设计很多围绕这个主题的变种模型结构。例如，复位门（或记忆门）的输出可以在多个隐藏单元之间共享。
另外，全局门（覆盖一整组的单元，例如一整层）和一个局部门（针对每单元）的乘积可以把全局控制和局部控制结合起来。但是经过调查研究发现，其它的一些变种结构在很多任务上都比不过LSTM和GRU (Greﬀ et al., 2015; Jozefowicz et al., 2015)。 Greﬀet al. (2015)发现一个关键因素是记忆门。 Jozefowiczet al. (2015)发现在LSTM的记忆门中加入一个偏置1，这个也有 Gers et al. (2000) 所提倡，可以使LSTM比所有已知的变种结构都厉害。

\section{长时依赖的优化}
\label{sec:10.11}
在\ref{sec:8.2.5}和\ref{sec:10.7}中我们介绍了当我们优化一个具有较长序列的RNN的时候，梯度会消失或者爆炸。

 Martens and Sutskever (2011)提出了一个非常有趣的想法：二阶导数可能在一阶导数消失的同时消失。
二阶优化算法可以简单的理解为将一阶导数除以二阶导数（在更高维数，由梯度乘以海森矩阵的逆）。如果二阶导数与一阶导数以类似的速率收缩，那么这两者的比率可保持相对恒定。不过基于二阶导数的优化方法有很多缺点，包括高计算成本，得适应较大的minibatch，有可能使结果陷入鞍点。 Martens and Sutskever (2011)发现使用二阶导数可以保证我们获得好的结果。 Sutskever et al. (2013)发现使用简单的方法也可以达到类似的效果，比如说小心初始化后的Nesterov动量法。更多细节请参考 Sutskever (2012)。在实际应用中，这两种方法都被SGD（甚至不带动量）所取代。
这是机器学习中一个将会持续讨论的主题，设计一个易于优化模型通常比设计出强大的优化算法容易。


\subsection{裁剪梯度}
\label{sec:10.11.1}
在\ref{sec:8.2.4}中，我们讨论了循环神经网络中强非线性函数经过多次递归计算之后，参数的梯度会趋向于爆炸或者消失。这个我们在图\ref{fig:8.3}和图\ref{fig:10.17}中进行了说明，在图\ref{fig:10.17}中我们可以看到目标函数（待学习参数的函数）所表示的三维图像中间有一个悬崖，原来宽阔平坦的区域被目标函数变化快的小区域隔开了，这个小区域形成了悬崖。

这导致的困难是，当参数梯度非常大时，我们用梯度下降法来更新参数时会把参数抛出很远，进入目标函数值较大的区域，我们之前做的工作都白费了。
梯度告诉我们，围绕当前参数值附近的区域（围绕该值的无穷小区域）内最速下降的方向。在这个无穷小区域之外，损失函数的值可能开始增加了，我们得取一个较小的学习速率来避免我们的参数直接进入损失函数值上升的区域。在实际应用中我们通常使用衰减的学习率来进行优化，衰减的速率要足够的慢，使相邻的步骤的学习率大致相同。
如果每个学习速率在目标函数相对线性的部分比较适合，那么当到达比较陡的部分的时候，同样的学习速率就会使得我们的要优化的参数进入目标函数较大的位置。


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=4in]{fig/chap10/10_17.PNG} 
   \caption{这是一个在循环神经网络上进行梯度裁剪的例子，在循环神经网络上有两个参数$\Vw$和$\Vb$。梯度裁剪可以使梯度下降法在损失函数陡峭的部分表现得更理智。这些陡峭的悬崖通常出现在循环神经网络的的损失函数近似线性部分的附近。循环神经网络在在多个时间步之后悬崖会变得越来越陡峭，因为权重矩阵在每个时间步都要乘以一下自己。（左边）未使用梯度裁剪的梯度下降法，直接越过了这个小山谷的底部，到达了悬崖对面一个梯度较大的的区域，这个区域已经超出了我们的坐标范围。（右边）基于梯度裁剪的梯度下降法对待陡峭的悬崖比较温和。它不会直接把参数更新到悬崖对面，它会限制学习率，所以它会把参数更新到离解附近的地方。该图取自 Pascanu et al. (2013)，使用已经获得了作者的许可}
   \label{fig:10_17}
\end{figure}


一个简单的解决方案已被大家使用多年：梯度截断。
这个想法衍生出不同的实例 (Mikolov, 2012;Pascanu et al., 2013)。
一种选择是在参数更新之前,将通过minibatch产生的参数的梯度的emph{每个元素}进行裁剪(Mikolov, 2012)。
另一种是在参数更新之前裁剪梯度$\Vg$的emph{范数}$\norm{ \Vg }$(Pascanu et al., 2013)：
\begin{align}
 \text{if}~ \norm{\Vg} &> v \\
 \Vg &\gets \frac{\Vg v}{\norm{g}},
\end{align}
其中$v$是范数上界，$\Vg$用来更新参数。
因为所有参数（包括不同的参数组，如权重和偏置）的梯度同时被被一个缩放因子规范化，所以第二个选择保证了每步更新方向仍在梯度方向上，但实验表明两个选择效果差不多。
虽然参数更新的方向与真实梯度的方向相同，经过梯度范数裁剪，参数更新的向量现在变得有界。
这种有界参数能避免在梯度爆炸时参数更新发生问题。
事实上，当梯度大小高于阈值时，即使使用简单的\emph{随机}更新参数，往往也可以取得一样好的效果。
如果爆炸非常严重，梯度为Inf或Nan（无穷大或不是一个数字），则可以随机选择大小为$v$的一步来更新参数，通常会参数离开数值不稳定的状态。
截断每个minibatch的梯度范数不会改变单个minibatch的梯度方向。
然而，经许多minibatch计算出梯度，然后使用范数裁剪，再把结果求平均值，这个均值不等于裁剪真实梯度（通过所有的实例计算梯度）的范数的结果。
如果某些样本计算出的梯度范数较大，以及与这些样本处于同一个minibatch的样本，他们对最终优化方向的贡献将消失。
和传统基于minibatch的梯度下降法不同，传统方法中，真实的梯度方向是等于所有minibatch梯度的平均。
换句话说，传统的随机梯度下降法使用梯度的的无偏估计，而与使用范数裁剪的梯度下降法中，我们通过经验引入了一个启发式偏置。
通过逐元素裁剪，更新的方向与真实梯度或基于minibatch的梯度不再一样，但是它仍然是一个使损失函数值下降的方向。
(Graves, 2013)还有提出在后向传播算法时截断梯度（相对于隐藏单元），但是没有人把这些变种梯度下降法进行比较。 我们推测，所有这些方法都具有类似的表现。


\subsection{通过正则化来引导信息流动}
\label{sec:10.11.2}
梯度裁剪可以帮助我们解决梯度下降法时遇到梯度爆炸的问题，但它不能帮助我们解决梯度消失问题。
为了解决梯度消失问题和更好地获得长时依赖信息，我们讨论了在循环结构展开的计算图中，创建一条路径，在路径上梯度累积的乘积等于1。
在\ref{sec:10.10}中已经提到过，实现这一点的一种方法是使用LSTM以及其他自循环和门限机制。
另一个方法是通过正则化来约束参数，然后来引导信息流动。
特别是，我们希望梯度向量$\nabla_{\Vh^{(t)}} L$在进行后向传播时可以维持它的幅度，即使损失函数只对序列最后部分的输出作出惩罚。
公式化这个想法，我们要使
\begin{align}
 (\nabla_{\Vh^{(t)}} L) \frac{\partial \Vh^{(t)}}{\partial \Vh^{(t-1)}}
\end{align}
与
\begin{align}
\nabla_{\Vh^{(t)}} L 
\end{align}
一样大。
在这个目标下， Pascanu et al. (2013)提出了下面的正则项：
\begin{align}
 \Omega = \sum_t \Bigg(  \frac{
 \norm{ (\nabla_{\Vh^{(t)}} L) \frac{\partial \Vh^{(t)}}{\partial \Vh^{(t-1)}}}}
 {\norm{\nabla_{\Vh^{(t)}} L}} -1 \Bigg)^2.
\end{align}
计算这一正则项的梯度可能会出现困难，但 Pascanu et al. (2013)提出可以将后向传播向量$\nabla_{\Vh^{(t)}} L$近似为一个常数（为了实现正则化，没有必要把它们向后传播）。
使用该正则项的实验表明，如果与范数启发式裁剪（处理梯度爆炸）相结合，这个正则项可以显著地增加RNN处理长时依赖的能力。
梯度裁剪特别重要，因为它保证了在发生梯度爆炸的时候RNN依然可以进行学习优化。
如果没有梯度裁剪，发生梯度爆炸后模型将无法进行后续的学习。

这种方法的一个主要弱点是，在处理含有冗余数据的任务时，如语言模型时，它并不像LSTM一样高效。


