%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% author:chaocraig %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% part:12.4-12.6   %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{自然语言处理}


自然语言处理（NLP）是让计算机使用人类的语言，包括英语或法语等。特殊设计的计算机程序通常读取和发出专门的语言，目的在允许通过简单的程序进行高效和明确的解析。使用的语言越自然，就越模糊与违反正规的描述。自然语言处理包括的应用，如：机器翻译，学习者一定要读入一句人类语言，然后使用另一个人类语言发出相等的句子。许多自然语言处理的应用，是基于自然语言中，定义文字（words）、字元（characters）或位元组（bytes）序列分布的机率分布的语言模型来发展的。


就像在本章中，所讨论的其他应用，非常通用的神经网路技术，可以很成功地应用的自然语言处理中。然而，为了达到优异的效能，以及规模化到大型应用中，一些领域特定的策略变得非常重要。为了建立一个有效率的自然语言模型，我们经常要使用特别为了处理序列资料的技术。在很多案例中，我们选择将自然语言是唯一序列的字，而不是一序列独立的字元或位元组。因为可能的字的总数很大，以字为基础的语言模型，一定要在一个极端高维度且稀疏分散的空间上操作。已经发展了好几种策略来让这样的空间模式，能在计算与统计意义上有效。


\subsection{n-grams}

一个语言的模型定义了一个在自然语言连续符号上的机率分布，根据这个模式如何背设计，一个符号可能是一个字、一个字元、或甚至一个位元组，符号总是离散的实体。最早成功的语言模型，是基于固定长度序列的符号之模型，称为 n-grams，一个 n-grams 是一串连续的 n 个符号。基于 n-grams 的模型，定义了在给定前面 n-1 个符号的第 n 个符号的条件机率。这个模型使用了这些条件分布的乘积，来定义在更长序列上的条件机率。

\begin{align}
\label{eq:necessary}
P(\bm{x}_{1},...\bm{x}_{T}) 
 = P(\bm{x}_{1},...\bm{x}_{n-1}) \prod_{t=n}^{T}P(\bm{x}_{t} | \bm{x}_{t-n+1},...\bm{x}_{t-1}) 
\end{align}


这个分解可以由概率的链式法来加以证明，而初始序列上的概率分布，则可以经由带有较小 n 值的不同模型来建模。


训练 n-gram 模型是很直接的，因为最大似然估计可以简单地计算，在训练集中，每个可能的 n gram 发生几次而得到估计值。基于 n-grams 的模型，已经有好几十年是统计语言模型的核心模块 (Jelinek and Mercer, 1980; Katz, 1987; Chen and Goodman, 1999)。


对于小的 n 值，模型有其特定的名称: 当 n = 1 时，称为一元语法(unigram)；当 n = 2 时，称为二元语法(bigram)；及当 n = 3 时，称为三元语法(trigram)。这些名称源于相应数字的拉丁前缀，同时，希腊后缀 “-gram" 表示所写的一些东西。通常我们同时训练 n-gram 模型和 n−1 gram 模型，这使得下列式子很容易计算概率:


\begin{align}
\label{eq:necessary}
P(\bm{x}_{t} | \bm{x}_{t-n+1},...\bm{x}_{t}) 
= \frac{P_n(\bm{x}_{t-n+1},...\bm{x}_{t}) }{P_{n-1}(\bm{x}_{t-n+1},...\bm{x}_{t-1}) }
\end{align}


简单地查找两个存好的概率值就能计算，为了在 Pn 中精确地再产生推论，当我们训练 \({P_{n-1}}\) 时，必须省略每个序列最后的字元。

举一个例子，我们演示用三元模型如何计算句子 "THE DOG RAN AWAY." 的概率。因为句子的开头没有上下文，所以句子的第一个词，不能通过上述默认的条件概率公式计算。替代方式是在句子的开头，我们必须使用词的边缘概率。因此，我们计算 \({P_{3}}\)(THE DOG RAN)，最后，是一个典型情况，可以使用条件分布 P(AWAY|DOG RAN)()来预测最后一个词。将算法这与公式 (12.6) 放在一起，我们得到:

\begin{align}
\label{eq:necessary}
P(\textrm{THE DOG RAN AWAY}) = {P_{3}}(\textrm{DOG RAN}){P_{3}}(\textmd{RAN AWAY})/{P_{2}}(\textmd{RAN})
\end{align}


对于 n-gram 模型之最大似然的基本限制，是即使元组(tuple) \({X_{t-n+1}}\), ..., \({X_{t}}\) 可能出现在测试集中，在许多情况下从训练集计数估计的 Pn ，在很多案例情形很可能为零，这可能会导致两种不同的灾难性后果。当 \({P_{n-1}}\) 为零时，该比率是未定义的，因此模型甚至不能产生有意义的输出。当 \({P_{n-1}}\) 非零而 Pn 为零时，测试样本的对数似然值为 − \(\infty\) 。为了要避免这种灾难性的后果，大多数 n-gram 模型采用某种形式的平滑(smoothing)，平滑技术将概率质量从观察到的元组，移转到类似的未观察到的元组，这些可以进一步参考 Chen and Goodman (1999) 的文献回顾和实证比较。其中一种基本技巧，是向所有可能的下一个符号值加入非零概率质量，这个方法可以被证明，是在具有均匀或狄利克雷（Dirichlet）分布的计数参数上的先验贝叶斯推断。另一个非常流行的想法，是形成包含高阶和低阶 n-gram 模型的混合模型， 其中高阶模型提供更多的容纳量，而低阶模型尽可能地避免计数值为零。如果上下文 \({X_{t-n+k}}\), ..., \({X_{t-1}}\) 的出现频率太小，而不能使用高阶模型，回退方法 (back-off methods) 就查找低阶 n-gram 的值。更正式地来说，它们通过使用上下文 \({X_{t-n+k}}\), ..., \({X_{t-1}}\) 来估计  \({X_{t}}\) 上的分布，并增加 k 直到找到足够可靠的估计值。


经典的 n-gram 模型特别容易引起维度灾难（the curse of dimensionality）。总共会有 \(|\mathbb{V}|^n\) 个可能的 n-grams ，且 \(|\mathbb{V}|\) 经常很大。即使有大量训练数据和适当的 n， 大多数的 n-gram 也不会出现在训练集中。经典的 n-gram 模型的一种观点，是视为执行最近邻查询。换句话说，它可以被视为局部非参数预测器，类似于 k-最近邻（k-nearest neighbors）。这些极端的局部预测器面临的统计问题，描述在 5.11.2 节之中。语言模型的问题，甚至比普通模型更严重，因为任何两个不同的词在独热（one-hot）向量空间中，具有彼此相同的距离。因此，难以大量利用来自任意“邻居”的信息——只有重复相同上下文的训练样本，对局部泛化有用。为了克服这些问题，语言模型必须能够在一个词和其他语义相似的词之间共享知识。

为了提高 n-gram 模型的统计效率，基于类的语言模型（Brown et al., 1992; Ney and Kneser, 1993; Niesler et al., 1998）引入词类别的概念，然后属于同一类别的词与词之间的统计强度可以共享。这个想法使用聚类算法，基于它们与其他词同时出现的频率，将该组词分成集群或类。之后，模型可以在条件棒的右侧使用词类 ID 而不是单个词的 ID，复合模型透过混合(或回退)词模型和类模型也是可能的。尽管词类提供了在序列之间泛化的方式，但是其中一些词被相同类的另一个词替换，使得这样的呈现方式，会丢失了很多信息。



\subsection{神經語言模型}


神经语言模型 (Neural Language Model, NLM) 是一種设计用来克服维度灾难的语言模型，它使用词的分布式表示对自然语言序列建模 (Bengio et al., 2001b)。和基于类 n-gram 模型不同，神经语言模型能夠识别两个词是相似的，而不丧失将每个词编码为互相不同的能力。神经语言模型共享一个词(及其上下文)和其他类似词和上下文之间的统计强度。模型学习到每个词的分布式表示，允许模型視具有共同特征的词是相似的，来实现这种共享。例如，如果词 dog 和词 cat 映射到共享许多属性的表示，则包含词 cat 的句子，其資訊有助於模型对包含词 dog 的句子做出预测，反之亦然。因为有很多这样的属性，所以可以發生许多泛化的方式，将信息从每个训练语句，传送到指数数量的语义相关语句。维度灾难需要模型泛化到句子长度是指数的一些句子。该模型透过将每个训练句子与指数数量的类似句子進行相关联，来克服这个维度灾难的问题。



