%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% author:hijeffery %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% part:14.0-14.6   %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{自编码器}
\label{chap:14}

\emph{自编码器}是神经网络的一种，用以训练来实现将输入复制到输出的目的。在其内部，有一个用编码表示输入的隐层$h$。自编码器可以看作由两部分组成：编码函数$h = f(x)$ 和进行信号重建的解码函数$r = g(h)$。 具体结构如图\ref{fig:14.1}所示。如果自编码器学到的结果仅仅是处处将$g(f(x)) = x$，则其没有起到任何作用。相反，自编码器被设计成了不能够完美的复制输入到输出的工作方式。通常他们被限定在只能够近似的复制，并且只复制能够与训练数据相像的输入。鉴于模型被强制执行输入的某些部分应当被复制，所以通常来讲，自编码器能够学习到数据的有用的特性。


\begin{figure}[htbp] %  figure placement: here, top, bottom, or page
   \centering
   \includegraphics[width=1in]{fig/chap14/14.1.png} 
   \caption{自编码器的基本结构，将输入信号$x$通过一个内部表征或者编码$h$ 映射到输出(也叫重建) $r$。 自编码器有两个子部分：编码器(将$x$映射到$h$)与解码器(将$h$映射到$r$)。}
   \label{fig:14.1}
\end{figure}

现代自编码器已经从执行特定的函数映射扩展到了执行随机映射$p_{encoder}(h|x)$ 和 $p_{decoder}(x|h)$。

自编码器的思想已经在神经网络研究领域存在了几十年。传统上来讲，自编码器是用来执行数据降维与特征学习的。近来，自编码器与隐变量模型的联系使得自编码器成为生成模型的研究前沿，详情见本书第\ref{chap:20}章介绍。自编码器可以视为前馈网络的一种特殊形式，并且可以用与其相同的方式进行训练，如以子集沿着反向传播计算的梯度下降方向求解。同一般的前馈网络不同的是，自编码器也可以用\emph{再循环}的方式进行训练，即对比原始输入的网络相应与重建信号作为输入的网络相应的差别。再循环技术被视为比反向传播算法更为接近生物特性的方法，但是其很少被出现在其他机器学习应用中。

\section{不完备自编码器}
将输入复制到输出听起来似乎无特别作用，但其实我们也不关心解码器的输出。事实上，我们期望通过训练自编码器实现复制输入的任务能够产生有实用意义的$h$。

一个从自编码器获取有效的特征的方法是将$h$限定到一个比$x$更低的维度上。特征维度比输入维度低的自编码器是\emph{不完备}的。学习一个不完备的特征迫使自编码器捕获到训练数据中的最为突出的特征信息。

学习过程可以简单的表述为最小化一个随时函数的形式：
\begin{equation}
	L(x,g(f(x)))
\end{equation}
其中，损失函数 $L$ 通过如最小化均方误差等限定$g(f(x))$ 与 $x$ 尽量相近。 

当解码器是线性函数，并且$L$ 是均方误差时，不完备自编码器学习到的生成子空间与PCA一致。此时，被用来执行复制任务的自编码器事实上附加学到了训练数据的主子空间。

因此，具有非线性编码函数$f$和非线性解码函数$g$的自编码器可以学习到更强有力的非线性泛化PCA。遗憾的是，如果编码与解码部分被给予太强能力的化，自编码器仍然仅仅完成复制输入到输出的功能而忽略了抽取数据分布等有效信息的能力。理论上说，我们可以设想一个只有一维编码的自编码器，其编码器具有强大的能力将每一个训练数据$x^{(i)}$表示成编码$i$。此外，解码器能够学习并将每一个整型值映射回特定的训练数值上。这种特殊情形在现实中不会出现，但其足够说明，一个被训练用来进行数据复制的自编码器，如果被赋予了过与强大的能力，并不会学习到任何与用的信息。