%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%author:Euniceu%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%part:14.7-14.9%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{14.7}


\frontmatter
\section{14.7 压缩自编码器}
压缩自编码器（Rifai et al.，2011a，B）在编码$h=f(x)$上引入了显示规则化项，它可以使f的导数尽可能小：

\begin{equation}
 \Omega(h) = \lambda \Bigg\| \frac{\partial f(x)}{\partial x} \Bigg\|_F^2 .
\end{equation}

惩罚项$\Omega(H)$是关于编码函数偏导的雅各比矩阵的平方弗罗贝尼乌斯范数（元素平方和）。

去噪自编码和压缩自编码之间是有联系的：Alain和bengio（2013）表明，在高斯噪声小的输入限制下，去噪重构误差相当于把x映射到$r=g(f(x))$的重构函数中的压缩惩罚项。换句话说，去噪自编码器使重构函数能抵抗小但有限大小的输入扰动，而压缩自编码器能使特征提取函数抵御极小的输入扰动。 当使用基于雅可比收缩惩罚项来预训练用于分类的特征$f(x)$时，将收缩惩罚项用于$f(x)$而非$g(f(x))$通常能得到更好的分类精度。如14.5.1节所讨论，应用于$f(x)$的收缩惩罚项与分数匹配也有紧密联系。

压缩这个名字来源于CAE扭曲空间的命名方式。具体而言，由于CAE被训练成来抵制其输入的扰动，它鼓励将输入点邻域映射到输出点的一个更小邻域。我们可以认为这是将输入邻域缩小到一个更小的输出邻域。

说得更清楚一点，CAE只是局部收缩——一个训练样本x的所有扰动都是被映射到$f(x)$附近。在全局范围内，两个不同的点$x$和$x’$ 可以映射到比原来两点的距离远得多的两个位置$f(x)$和$f(x')$。f被扩展到数据集合的中间或远离数据集是合理的（见图14.7一维玩具例子）。当惩罚项$\Omega(H)$应用于sigmoid单元时，一个缩小雅可比的简易方法是令sigmoid 单元饱和到0或1。这可以促使CAE使用sigmoid的极值编码输入，这样有可能被解释为一个二进制编码。它还确保了CAE的编码值能在大部分的由sigmoid隐藏单元张成的超立方体上传播。

我们可以将在一个点$x$处的雅可比矩阵$J$认为是一个非线性编码器$f(x)$的近似，它可以作为一个线性算子。这使我们能够更加正式地使用“压缩”这个词。在线性算子理论中，如果$Jx$范数对于所有的单位范数$x$都小于等于1，那么这个线性算子就被称为是收缩的。换句话说， 如果J收缩了单元球，那么它就是收缩的。我们可以认为CAE是每个训练点$x$ 处的局部线性近似$f(x)$的弗罗贝纽斯范数的惩罚项，这是为了促使每一个局部线性算子都具有收缩性。

如在图14.6中所描述，正则自编码器学习通过平衡两种相反的力来学习流形。在CAE中，这两个反力是重构误差和收缩惩罚项$\Omega(H)$。独立的重构误差会促使CAE学习一个恒等函数。而单独的收缩惩罚项会促使CAE学习关于$x$的恒定特征。这两股力量之间的折衷产生了一个有着很小$\frac{\partial f(x)}{\partial x}$导数的自编码器。只有一小部分反映输入方向的隐藏单元单位可能有显著的导数。

CAE的目的是学习数据的流形结构。关于大$Jx$的方向快速地改变$h$，所以这可能近似于流形切平面的方向。Rifai 等人（2011A）（2011b）的实验显示，训练CAE会导致$J$中的大部分奇异值大小降到1以下，因而导致了收缩性。然而，一些奇异值仍保持在1以上，因为重构误差惩罚项鼓励CAE对最大的局部方差进行编码。相应于最大奇异值的方向被解释为压缩自编码器学习到的切线方向。理想情况下，这些切线方向应对应于数据中的真实变化。例如，应用于图像的CAE学习到的切向量应该显示出图像中的对象逐渐变化姿态所产生的变化，如图14.6所示。如图14.10所示，实验得到的奇异向量的可视化似乎真的对应于输入图像有意义的转换。

关于CAE正则化准则的一个实际问题是：虽然计算一个单隐藏层自编码器很容易，但是计算更深层的自编码器时代价就变得高多了。Rifai等人（2011A）使用的策略是分别训练一系列的单层自编码器，每一个当前的训练都是对前一层自编码器的隐藏层的重建。将这些自编码器组合，就成了一个深层自编码器。因为每一层都是分别训练成了局部收缩，所以深自编码器也是收缩的。这样得到的结果与通过联合训练一个带有雅可比惩罚项的整体深度模型得到的结果是不一样的，但它也能够捕获许多理想的定性特征。


另一个实际问题是：如果我们不对解码器强加一些约束，收缩惩罚项可能导致无用的结果。例如，编码器可能由输入乘上一个很小的常数$\epsilon$组成，而解码器就除以这个常数$\epsilon$。当常数$\epsilon$趋于0时，编码器会驱使收缩惩罚项$\Omega(H)$在没有学到任何有关分布的信息的情况下趋于0。同时，解码器保持完美的重构。在Rifai等人。（2011A）中，通过捆绑$f$和$g$来防止出现这种情况。$f$和$g$都是由仿射变化后进行按元素进行的非线性变换组成的标准神经网络层，因此将g的权重矩阵设置为f权重矩阵的转置是很直接的。

\section{14.8 预测稀疏分解}
预测稀疏分解（PSD）是一个由稀疏编码和参数自编码器组成的混合模型（kavukcuoglu et al.，2008）。参数自编码器被训练来预测迭代推断的输出。PSD已应用于图像和视频中对象识别的无监督特征学习（Kavukcuoglu et al., 2009, 2010; Jarrett et al., 2009; Farabet et al., 2011），在音频中也有应用（Henaff et al., 2011）。该模型包括一个编码器$f(x)$和一个解码器$g(h)$，二者都是参数化的。在训练过程中，$h$是由优化算法控制。训练过程是最小化

\begin{equation}
 \| x - g(h) \| ^2 + \lambda | h |_1 + \gamma \| h - f(x) \|^2.
\end{equation}

与稀疏编码一样，训练算法交替地进行关于$h$的最小化和关于模型参数的最小化。相对于$h$的最小化速度更快，因为$f(x)$提供了很好的$h$初始值，并且损失函数始终将h约束在$f(x)$附近。简单的梯度下降（甚至可以少到十步）就可以获得理想的$h$值。

PSD的训练过程不同于先训练稀疏编码模型，然后再训练$f(x)$来预测的稀疏编码的特征值。PSD的训练过程规则化解码器，使之能使用使$f(x)$可以推断出良好编码值的参数。

预测稀疏编码是学习近似推断的一个例子。在19.5节中，这个话题将进一步展开。第19章中展示的工具确保了PSD可以解释为通过最大化模型的对数似然下界来训练一个有向稀疏编码概率模型。

在PSD的实际应用中，迭代优化只是训练中使用。模型确定好后，参数编码器$f$是用来计算所学特征的。与通过梯度下降来推断$h$相比，评估$f$的计算成本就容易一些。因为$f$是含参可微函数，PSD模型可堆叠，也被用于初始化一个用其他规则训练的深度网络。

\section{14.9 自动编码器的应用}
自动编码器已成功地应用于降维和信息检索任务。降维是表示学习和深度学习最早的应用之一。它是研究自动编码器早期的动机之一。例如，Hinton和salakhutdinov（2006）训练一个堆叠RBM，然后利用它们的权重来初始化隐藏层节点逐渐减少的深度自编码器，节点数最终在30个单元处达到瓶颈。最终的代码产生的重构误差比30维的PCA产生的小，而且它所学到的表征更容易定性解释及进行分类，这些类别表现为良好的分离集群。

低维表示可以提高许多任务的性能，如分类。较小的空间模型消耗更少的内存和运行时间。根据Salakhutdinov、Hinton （2007b）和Torralba等人（2008）的观察，降维的许多形式是语义上与彼此相关联的例子。映射到低维空间增强了泛化能力提示了这一点。

从降维中比普通任务获得更多好处的任务是信息检索。信息检索的任务是在一个数据库中查找类似条目。这项任务像其他任务一样，从降维中获得一般性的益处，但它同时也获得了额外的益处，即在某些低维空间中进行搜索将会更高效。具体地讲，如果我们训练一个降维算法来生成低维二进制代码，然后我们可以将所有数据条目从哈希表中映射到二进制编码向量条目。这个哈希表允许我们通过返回具有相同的二进制代码的所有数据库条目来进行信息检索。我们也可以非常有效地搜索稍微不太相似的条目，只是通过翻转个别位的查询编码。这种通过降维和二值化的信息检索方法称为语义哈希（Salakhutdinov 和 Hinton, 2007b, 2009b），并已经应用于文本输入（Salakhutdinov和Hinton，2007B，2009b）和图像（Torralba et al., 2008; Weiss et al., 2008; Krizhevsky and Hinton, 2011）。

为使语义哈希产生二进制码，通常在最后一层使用sigmoid编码函数。对于所有的输入值，Sigmoid单元必须训练到饱和状态，或者接近0或者接近1。做到这点的一个技巧使是：只需在训练过程中，在sigmoid非线性单元之前加入加性噪声。噪声的大小应随着时间而增加。为了对抗这种噪音并保持尽可能多的信息，网络必须增加对sigmoid函数的输入幅度，直到函数饱和。

学习哈希函数的想法已经在几个方向上进行了进一步探索，包括训练表示以优化损失来更直接地关系到在哈希表中查找附近实例任务的想法（Norouzi and Fleet, 2011）。
\end{document}
