%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% author:KaiserW %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% part:5.7-5.11  %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{监督学习算法}
\label{sec:5.7}
前承\ref{sec:5.1.3}节，有监督学习（Supervised Learning）简单来讲就是一种学习算法，它会学着在某些输入和某些输出之间建立关联，这些输入\textbf{x}和输出\textbf{y}来自于训练集中的样本。很多时候，输出\textbf{y}很难自动采集，而必须由一位人工“监督者”(supervisor)提供，当然即便训练集的拟合目标已经自动采集完成，“监督学习”的名称仍然适用。
\subsection{概率监督学习}
\label{sec.5.7.1}
本书提到的大多数监督学习算法都是基于对概率分布$p(y|x)$的预测。我们可以简单地应用最大似然估计(maximum likelihood estimation)来找到分布$p(y|x;\theta)$参数族的最佳参数向量$\theta$。

已知线性回归(linear regression)对应参数族
\begin{equation}
	p(y|x;\theta) = \mathbb{N} (y;\theta^{T}, \textbf{\textit{I}})
  	\label{form:5.80}
\end{equation}

通过定义不同族的概率分布，我们可以将线性回归推广到分类(classification)情景。如果我们有两个类别，类0和类1，那么接下来只需确定其中一个类的的概率就可以了。类1的概率自然也就决定了类0的概率，因为两个概率值相加必然为1.
基于平均值将实数域上的正态分布进行参数化，这一分布我们也用于线性回归，这里的平均值可以是任意值。但是二元变量的分布则更加复杂一些，因为其平均值必然始终落在0和1之间。一种解决方案是应用逻辑函数（logistic function， 也称sigmoid函数）将线性函数的输出值挤压到(0,1)区间，转换后的值可以理解为是一个概率：
\begin{equation}
	p(y=1|x;\theta) = \sigma (\theta^{T}x)
  	\label{form:5.80}
\end{equation}

这一方法即是逻辑回归(logistic regression)，这名字有些古怪，因为我们实际上用这个模型做分类而不是回归。
对于线性回归，我们可以解正规方程(normal equations)以求得最优权重。而逻辑回归就要复杂一些，它的最优权重没有解析解。我们只能通过最大化对数似然率(log-likelihood)来逼近最优解，具体的策略是，应用梯度下降法(gradient descent)使负对数似然率(negative log-likelihood, NLL)最小化。

这一策略基本可以应用在任何监督学习问题中：对于正确类型的输入/输出变量，写下其条件概率分布的参数族。

\subsection{支持向量机}
\label{sec:5.7.2}

支持向量机(Boser et al., 1992; Cortes and Vapnik, 1995)是最具影响力的监督学习方法之一。该方法与逻辑回归很相似，因为都是由线性函数$\omega^{T}x + b$所驱动。不同于逻辑回归，支持向量机(Support Vector Machine, SVM)并不提供概率值，只有分类结果。当$\omega^{T}x + b$为正，SVM预测为正类；同理当$\omega^{T}x + b$为负，则预测为负类。

支持向量机的关键创新点是\textbf{核技巧}(kernel trick)。核技巧观察到很多机器学习算法可以写作样本的点乘积。例如，支持向量机所用的线性函数可以写作形如
\begin{equation}
	\omega^{T}x + b = b + \sum_{i=1}^{m}{\alpha_{i}x^{T} x^{(i)}} 
    \label{form:5.81}
\end{equation}

这里$x^{(i)}$是一个训练样本，$\alpha$是系数矢量。

以这种方式重写学习算法之后，我们便可以用特征函数$\phi(x)$的输出和函数$k(\textbf{x}, \textbf{x}^{(i)})=\phi(\textbf{x})\cdot\phi(\textbf{x}^{(i)})$替代$\textbf{x}$，其中的$k(\textbf{x}, \textbf{x}^{(i)})=\phi(x)\cdot\phi(\textbf{x}^{(i)})$就叫做\textbf{核}(kernel)。$\cdot$操作符表示与$\phi(x)^{T}\phi(\textbf{x}^{(i)})$类似的内积。在有些特征空间中，我们可能无法使用真正的矢量内积；在有些无限多维的空间中，我们需要使用其他类型的内积，比如基于积分而不是加法的内积。此类内积的完整推导已经超出了本书的范畴。

用核替代了点积之后，我们可以用以下函数做预测
\begin{equation}
	f(x) = b + \sum{i}^{}{\alpha_{i}k(x,x^{(i}}
	\label{form:5.82}
\end{equation}

此函数对$textbf{x}$是非线性的，但是$\phi(\textbf{x})$和$f(\textbf{x}$之间是线性关系。并且$\alpha$和$f(\textbf{x}$也是线性关系。以下过程与基于核的方程都是严格等效的：对所有输入应用$\phi(\textbf{x}$，然后在新的变换空间中学习线性模型。

核技巧的强大有两重原因。首先，它允许我们并使用保证有效收敛的凸优化(convex optimization)技术，把对$x$的非线性函数当作线性的来学习。这是因为我们认为$\phi$是不变的，只优化$\alpha$，换言之，优化算法可以把决策方程在另一个空间中看作线性的。其次，相比于直接构建两个$\phi(\textbf{x})$矢量并显式求点积，核函数$k$的计算效率往往更高。

某些情况下，$\phi(\textbf{x})$甚至可以是无限维的，直接的显式求解将导致无穷的计算消耗。多数情况下，$k(\textbf{x}, \textbf{x'})$是$\textbf{x}$的非线性可解函数，即使$\phi(\textbf{x})$不可解。作为无限维特征空间中可解核的例子，我们构建一个特征映射，从非负整数x到$\phi(\textbf{x})$，设想该映射返回一个包含x个1及无穷多个0的矢量。我们可以写一个核函数$k(\textbf{x}, \textbf{x'}) = min(x, x^{i})$，这与无限维的点积严格等价。

最常用的核是\textbf{高斯核}(Gaussian kernel)
\begin{equation}
	k(u, v) = \mathbb{N}(u-v;0, \sigma^{2}I)
\end{equation}
$\mathbb{N}(x;\mu,\Sigma)$是标准正态密度。这个核也被称为径向基函数(radius basis function, RBF)核，因为其值在$v$空间中沿着$u$向外辐射而减小。高斯核对应着无限维空间里的点积，但是这一空间中的推导不像之前整数核的例子那样直观。

我们可以认为高斯核实现的是一种模板匹配(template matching)。一个与训练标签$y$相关的训练样本$x$构成了类$y$的一个模板。当测试点$x'$与$x$的欧几里得距离(Euclidean distance)很近的时候，高斯核有一个很大的响应，表示$x'$与$x$模板很相似。这一模型给相关训练标签$y$的权重很高。总体上看，预测是把基于对应模板样本(training example)进行过加权的训练标签(training label)组合了起来。

支持向量机并非唯一经由核技巧加强的算法，很多其他的线性模型都可以通过这种方式加强。这类搭载了核的算法也被称作核机器(kernel machine)或核方法(kernel method)。

核机器的最大缺在于，评估决策函数的计算量与训练样本数量呈线性关系，因为第$i$个样本向决策函数提供了$\alpha_{i}k(x,x^{(i)})$。支持向量机可以通过学习一个主要由0构成的矢量$alpha$来缓解这一弊端，对一个新样本做分类，只需要评估$\alpha_{i}\neq0$的样本，这些训练样本就是\textbf{支持向量}(support vector)。

核机器面临的另一大困难就是处理大数据时的超高计算资源消耗，我们将在\ref{sec:5.9}节重新审视该问题。普通的核机器很难提高适用性，我们将在\ref{sec:5.11}节重点讨论。现代深度学习的诞生正是为了突破这些局限，而当下的深度学习“复兴”正是始自Hinton et al(2006)展现了在MNIST数据集上，神经网络比RBF核支持向量机表现的更有力。

\subsection{其他简单的监督学习算法}
\label{sec:5.7.3}

我们已经简单了解过另一非概率的(non-probabilistic)监督学习算法，\textbf{近邻回归}(nearest neighbor regression)。更一般地来讲，k近邻（k-nearest neighbors)是一系列可用于分类和回归的技术。作为无参数学习算法，k近邻不为固定的参数量所限。我们通常认为k近邻算法没有任何参数，而是对训练数据施加了一个简单的函数。实际上k近邻甚至没有真正的训练或学习过程，在测试过程中，当我们想要对一个新测试输入$x$产生新输出$y$的时候，我们直接从训练数据$X$里找到离$x$最近的点，然后返回训练集对应$y$的平均值。在一个监督学习算法里，只要我们能定义出$y$的平均值，这个方法就是好用的。

在分类问题中，我们可以对独热码矢量$\textbf{c}$做平均，其中$c_{y}=1$且其他i值的$c_{i}=0$。对于独热码的平均可以理解为关于类的概率分布。作为无参数学习算法，k近邻的性能属于非常高的。比如当我们有一个多分类任务，并且用0-1的损失值衡量分类表现的时候，随着样本数量趋向无穷多，1近邻(k=1)将收敛至2倍贝叶斯误差(Bayes error)。超过贝叶斯误差的部分是因为，当存在两个距离相同的近邻时，我们只能随机选择其中一个，如此就割裂了两个点之间的关联。如果训练数据无穷多，每个测试点x周围都有无穷多个训练集近邻与之0距离。若是让算法纳入所有这些近邻点而不是随机选取的话，最终就能收敛至贝叶斯误差。当训练集很大的情况下，k近邻的高性能特性使其能够达到极高精度。但是随之而来的计算消耗也是巨大的，且如果训练集较小，预测的泛用性会很差。

k近邻算法的一大缺点就是，它不能自主发现有的特征比其他特征的判别力度更强。想象我们在有$x\subset\mathbb{R}^{100}$空间中有一个回归任务，x来自于各向同性的高斯分布，但是只有1个变量$x_{1}$与输出有关。更进一步地，假设这一特征变量直接决定输出，比如$y=x_{1}$恒成立。但近邻回归却无法探索出这一简单的模式，多数点$x$的最近邻判别仍将受到$x_{2}$到$x_{100}$的众多特征影响，而不是仅有$x_{1}$单独决定，于是小规模训练集的输出基本会是个随机变量。

\begin{figure}[htbp]
   \centering
   \includegraphics[width=6in]{fig/chap5/5_7.png} 
   \caption{决策树的运作机理。（上图）树的每个节点选择将输入样本送至左子节点(0)或右子节点(1)。中间节点画作圆圈，叶子节点画作方块。每个节点都以二级制字符串表示其在树中的位置，在其父节点的识别编码后增补字节(0=左或上，1=右或下)。（下图）树将输入空间划分成不同区域。这个2D平面显示了决策树如何划分$R^{2}$，中间节点画在给样本分类的分割线上，叶子节点画在样本对应区域的中心。结果是一个分段等值函数，每个叶子节点都相当于一段。每个叶子节点都需要至少一个样本才能定义，所以决策树不能学习到局部最大值比样本数量还多的函数。}
   \label{fig:5_7}
\end{figure}

另有一类学习算法也将输入孔建分成了不同的区域，并且每个域自有独立的参数，这就是\textbf{决策树}(decision tree, Breiman et al.)及其变种。如\ref{fig:5_7}所示，决策树的每个节点都对应输入空间里的不同区域，每个区域再被中间节点细分为子区域，并形成子节点（通常基于轴向切分）。空间由此继续细分为不重叠的区域，建立起叶子节点(leaf node)与输入区域(input region)之间的一一对应。每个叶子节点通常将来自同一输入区域的点映射到相同的输出。

决策树一般通过特殊的算法训练，这些算法超出了本书的范畴。如果一个学习算法可以学习任意规模的树，那么我们就可以认为该算法是无参数的，尽管在实际应用中决策树通常都经过基于规模限制的正则化(regularization)以转换为有参数模型。

决策树在使用中经常经过轴向切分(axis-aligned split)，每个节点对应一个不变的输出，很多逻辑回归都能轻易解决的问题对决策树却可能很困难。例如，如果我们有一个二分类问题，当$x_{2} > x_{1}$时为正类，那么决策边界可能就不是轴向的。于是决策树就需要结合多个节点来估测决策边界，估测过程是调用一个阶跃函数(step function)在正确的决策函数边界上做等值、轴向的反复走步。

如前所述，近邻预测器和决策树存在很多限制，然而当计算资源有限的时候它们还是非常实用的。通过思考更复杂的高级算法与k近邻、决策树之间的相似与差异，我们可以获得发现更精巧算法的直觉。

关于传统监督学习算法的材料，可查看Murphy(2012), Bishop(2006), Hastie et al.(2001)或者其他机器学习课本。

\section{非监督学习算法}
\label{sec:5.8}

前承\ref{sec:5.1.3}节，无监督算法只有“特征”而没有监督信号。监督算法和非监督算法之间并没有明确严苛的区别，因为一个值属于特征还是监督者提供的标签，这本来也不存在客观定义。一般来讲，无监督学习指的是那些不借助人为干预或外部信息，直接从分布中提取信息的算法。与其高度相关的概念有：密度估计，样例提取，降噪，流形，聚类等。

有个经典的无监督学习任务，就是寻找数据的“最佳”表征。这里的“最佳”指的是尽可能多的保留$x$的信息，同时保证表征的形式比$x$本身更加简单。

定义更简单的表征可以有多种方式。三种最常见的是低维表征(low-dimensional representation)、稀疏表征(sparse representation)和独立(independent representation)表征。低维表征试图尽量压缩信息；稀疏表征(Barlow, 1989;Olshausen and Field, 1996;Hinton and Ghahramani, 1997)将数据集嵌入到另一个主要由0构成的表征里。使用稀疏表征往往要升维，整体结构是将数据分发至表征空间的各个轴上；独立表征则试图理清数据变化背后的来源，以使表征的每个维度之间都服从统计独立。

当然这三个标准并非互斥关系。低维表征常常降低元素之间的依赖性，这是因为压缩信息量的一种常用方法就是识别并剔除数据中的冗余关系。冗余的降低让降维算法可以直接忽略少部分信息以达到更大的压缩程度。

表征(representation)是深度学习的核心命题，也是本书的核心命题。在这一节中，我们开发了几个表征学习的例子，这些示例算法将展示如何实现上面所有的三个判据。之后的章节也将介绍更多从不同角度演绎这三大判据的表征学习算法以及其他判据。

\subsection{主成分分析}
\label{sec:5.8.1}

在\ref{sec:2.12}中，我们已经了解到，主成分分析(PCA, principal component analysis)算法提供了一种压缩数据的方法。我们同样把PCA看作是一种无监督学习算法，它从数据中学习表征，这样的表征基于以上提到的两种判据。PCA学习维数低于初始输入的表征，也学习一个元素与其他元素线性无关的表征。这正是统计独立表征学习的第一步。为了获得完全的独立性，表征学习算法必须将变量之间的非线性关系也识别并剔除掉。

\begin{figure}[htbp]
   \centering
   \includegraphics[width=6in]{fig/chap5/5_8.png} 
   \caption{PCA学习一种线性投影方式，使变化最大的方向与新空间的轴平行。（左图）初始数据由样本$x$构成，在这一空间中，数据的变化可能与坐标轴不平行。（右图）经过变换的数据$z=x^{T}W$现在基上是沿着轴$z_1$变化。第二大的变化方向则沿着$z_2$.}
   \label{fig:5_8}
\end{figure}

PCA学习数据的正交线性变换，将输入$x$投影到表征$z$，如\ref{fig:5_8}所示。在\ref{sec:2.12}中已经介绍过，我们可以学习构建原始数据的“最佳”一维表征（最小二乘法），这一表征其实就对应数据的第一个主成分。接着我们可以把PCA用作一种简单有效的降维方法来尽可能多的保留数据中的信息（同样根据最小二乘法）。接下来，我们将学习PCA表征如何与原始数据表征\textit{X}“去相关”。

考虑一个$m\times n$的设计矩阵$X$。假设数据的平均值为0，$E[x]=0$。如果非零，只需在预处理中将所有数据同时减去现有均值即可。

$X$的无偏协方差矩阵写作：
\begin{equation}
	Var[x] = \frac{1}{m-1}X^{T}X
   \label{form:5.85}
\end{equation}

PCA找到一个表征（通过线性变换）$z=x^{T}W$使$Var[z]$为对角阵。
在\ref{sec:2.12}中，我们已知设计矩阵$X$的主成分由$X^{T}X$的特征向量给出：
\begin{equation}
	X^{T}X = W\Lambda W^{T}
   \label{form:5.86}
\end{equation}

本节，我们将挖掘另一种主成分的推导方法。主成分也可以由奇异值分解(SVD)得到。特别地，他们就是$X$的右奇异向量。为证此理，设$W$为分解$X=U\Sigma W^{T}$的右奇异向量。再将$W$的原始特征向量还原为特征向量基。
\begin{equation}
	X^{T}X = (U\Sigma W^{T})^{T} U\Sigma W^{T} = W\Sigma^{2}W^{T}
   \label{form:5.87}
\end{equation}

SVD可以帮助我们证明PCA可以得到对角的$Var[z]$。对$X$做SVD，我们可以将$X$的方差写作：
\begin{align}
	Var[x] & = \frac{1}{m-1}X^{T}X\\
	&= \frac{1}{m-1}(U\Sigma W^T)^T U\Sigma W^T\\
	&=\frac{1}{m-1}W\Sigma ^T U^T U \Sigma W^T\\
	&=\frac{1}{m-1}W \Sigma ^2 W^T\\
\end{align}

上面的推导中我们用到了$U^T U = I$因为奇异值分解的矩阵$U$被定义为正交矩阵。下面证明如果我们取$z=x^T W$，可以确定$z$的协方差是对角矩阵：
\begin{align}
	Var[x] & = \frac{1}{m-1}Z^{T}Z\\
	&= \frac{1}{m-1}W^T X^T X W\\
	&=\frac{1}{m-1}W^T W \Sigma ^2 W W^T\\
	&=\frac{1}{m-1}\Sigma ^2\\
\end{align}
这次我们利用了$W^T W = I$，同样来自SVD的定义。

以上分析展示了当我们通过线性变换$W$将数据$x$投影到$z$的时候，所生成的表征拥有对角协方差矩阵($\Sigma ^2$)，也就说明了$z$的每个元素都是相互独立的。

将数据变换成为元素相互独立的表征，这种能力是PCA的重要属性，也展现了表征的作用在于试图理清数据背后未知的变化因子。在PCA当中，这种理顺工作的形式是旋转输入空间使变化的主轴与新的表征空间$z$的基向量平行。

相关性(correlation)是数据元素之间依赖性(dependency)的重要组成部分，我们也同样对表征学习如何理顺更复杂的特征依赖感兴趣。为此，除了简单的线性变换，我们还有更多工作要做。

\subsection{k平均聚类}
\label{sec:5.8.2}

另一个简单的表征学习实例是k平均聚类(k-means clustering)。k平均算法将训练集分为k个组，每个组内部的样本相互都很接近。我们可以进一步认为该算法提供了一个k维的o独热码向量$h$来表征输入$x$。如果$x$属于一个类$i$，则$h_i=1$且$h$的其他元均为0。

k平均聚类提供的独热码正是稀疏表征的一个实例，因为每个输入的绝大多数元都是0。之后我们会接触到其他算法，它们能够学习更灵活的稀疏表征——每个输入不止有一个元为1。独热码是稀疏矩阵的极端情况，因为它丢失了分布表征的很多优点。但同时独热码也赋予了算法统计上的优势（天然地表达出同类样本相互接近）而且也带来了计算优势，整个表征可由一个整数代表。

k平均算法首先对不同的值生成k个不同的“质心”${\mu^{(1)},...,\mu^{(k)}}$，然后交替进行以下两个步骤直至收敛。步骤一：每个训练样本都被分配到某个组i当中，i就是与之最近的质心$\mu^{(i)}$的索引。步骤二：每个质心$\mu^{(i)}$更新为本组内所有训练样本$x^{(j)}$的平均值。

聚类问题的一大困难就是其天然具有病态性质，也就是并不存在一个绝对判据来衡量数据的聚类结果与真实世界多么符合。我们可以聚类的某些性质诸如分组质心到各成员的平均欧几里得距离。这可以表示我们能够在多大程度上从聚类重建原始训练数据，但是我们仍未知道聚类与真实世界的性质相去几何。

更有甚者，可能存在多种聚类结果都与真实世界的同一性质吻合良好，我们找到的可能是一个与某种性质相关、同样有效却与聚类任务本质无关的另一聚类。比如，假设我们正在一个由红色卡车、红色汽车、灰色卡车和灰色汽车组成的数据集上同时运行两个聚类算法。如果我们让两个算法各自进行二元聚类，一个算法可能按照外形分为汽车和卡车；另一算法可能按照颜色分为红色交通工具和灰色交通工具。如果再来第三个算法自动指定聚类数量，又可能分为四组——红色卡车、红色汽车、灰色卡车和灰色汽车。新算法至少同时捕捉到了两种性质的差异，但是却失去了相似性的信息。红色汽车与灰色汽车分属不同组别。聚类算法的输出并没有告诉我们，相比于灰色卡车，红色汽车与灰色汽车更接近，因为两者在颜色/外形上都不同，虽然我们都知道但算法却无法识别。

比起独热码，我们更倾向于分布表征，以上这一问题正表现出了部分原因。分布表征可以使每个交通工具具有两个性质——一个表征颜色一个表征外形（汽车/卡车）。这还未必就是最佳表征（学习算法怎么知道我们感兴趣的是颜色和外形，而非制造商和车龄呢？），但是已经减轻了算法猜测应该关照哪些属性的计算负担。并且由此我们将能够以高细粒度的方式来衡量物体之间的相似度——对比多种属性而非匹配单一属性。

\section{随机梯度下降法}
\label{sec:5.9}

几乎所有的深度学习背后都有同一种算法在支撑着：随机梯度下降法(Stochastic Gradient Descent, SGD)。\ref{sec:4.3}介绍过梯度下降法，随机梯度下降法则是其延伸和扩展。

机器学习中有一个反复出现的问题就是，高普适性需要大训练集，而大数据集又会消耗大量计算资源。

机器学习所用的代价函数(cost function)往往都是单个训练样本损失函数(loss function)的总和。例如，训练数据的负条件对数概率可以写作：
\begin{equation}
	J(\theta) = E_{x,y~\hat{p}_{data}}L(x,y,\theta) = \frac{1}{m}\sum_{i=1}^m L(x^{(i)},y^{(i)},\theta)
   \label{form:5.88}
\end{equation}

L是单个样本的损失 $L(x,y,\theta) = -log p(y|x;\theta)$。

对于这些累加的损失函数，梯度下降法需要计算
\begin{equation}
	\nabla_{\theta}J(\theta) = \sum_{i=1}^m \nabla_{\theta}L(x^{(i)},y^{(i)},\theta)
   	\label{form:5.89}
\end{equation}

这一运算的计算复杂度是$O(m)$。当训练集的规模增长到十亿级，单单一次梯度下降的耗时就长的不可接受了。

随机梯度下降法的内涵在于，梯度是一种期望值，而期望值可以通过一小撮样本来估计。特别的，在算法的每一步，我们可以从样本中均匀采样，构建一个小批量(minibatch)：$B={x^{1},...,x^{m'}}$。小批量的规模$m'$相对样本总量来讲是很小的，一般就是从1到数百。关键是，哪怕训练集的规模$m$在增长，$m'$也是固定的。仅用数百个样本，我们就可以训练十亿量级的训练集。

使用小批量$B$中的样本，梯度预测的形式为
\begin{equation}
	g = \frac{m'}{1} \nabla_{\theta} \sum_{i=1}^{m'} L(x^{(i)}, y^{(i)}, \theta)
   	\label{form:5.90}
\end{equation}

随机梯度下降法即以如下方式下降：
\begin{equation}
	\theta \leftarrow \theta - \epsilon g
    \label{form:5.91}
\end{equation}

其中$\epsilon$为学习率(learning rate)。

梯度下降法一般常被认为是缓慢而不可靠的。过去，在非凸优化中应用梯度下降也被认为是不理智、不合适的。如今我们知道本书第二部分介绍的机器学习模型，经过梯度下降法训练之后，运行的非常好。优化算法或许连局部最小值也不敢保证能达到，但是能够在够短的时间内找到相对很低的值，这就够用了。

在深度学习以外，随机梯度下降法也有很多重要应用。这也是通过极大数据集训练线性模型的主要方法。对于固定的模型规模，每一次SGD更新所带来的计算消耗与训练数据集的规模$m$无关。在实践中，随着训练集增大，我们经常使用更大的模型，当然也并非强制的。达到收敛所需的更新次数，通常随着训练集规模而增加。然而，随着$m$趋向无穷大，模型会最终在SGD对训练集的每个样本做采样之前，就收敛至最小测试误差。越来越大的$m$并不会延长误差最小化的训练时间。从这个角度来看，训练SGD的代价作为$m$的函数，会趋近于$O(1)$。

在深度学习到来之前，学习非线性模型的主要方法是结合线性模型与核方法。许多核学习算法需要构建$m \times m$的矩阵$G_{i,j}=k(x^{(i)}, x^{(j)})$。构建此类矩阵的计算消耗是$O(m^2)$，对于十亿量级的训练集来说，这显然不可行。从2006年开始，深度学习逐渐获得学界青睐，因为相对其他算法，深度学习可以在中等规模的数据集（万级）上获得更好的推广性。不久之后，深度学习就在业界获得更多的关注，因为它使得在大数据集上训练非线性模型成为可能。

随机梯度下降法及其优化增强，将在第八章中进一步探讨。

\section{构建机器学习算法}
\label{sec:5.10}

几乎所有的深度学习算法都可以描述为一个简单秘方的特例，该秘方的成分为：数据集规范，代价函数，优化方法和模型。

例如，一个线性回归算法包含$X$和$y$，代价函数
\begin{equation}
	 J(\omega , b) = -E_{x,y p_{data}} \log p_{model}(y|x),
    \label{form:5.92}
\end{equation}

模型规范$p_{model}(y|x)=N(y;x^T\omega+b,1)$，且多数情况下，优化算法的定义来自：通过正规方程求解代价函数的梯度为零之处。

其实我们可以任意替换将这些成分，也就得到了算法的很多变形。

代价函数通常至少含有一个项，使学习进程执行统计预测。最常见的代价函数是负对数，所以最小化代价函数也就等价于最大化似然率预测。

代价函数也可能包含附加项，比如正则项。举例来讲，我们可以给线性回归的代价函数增添一项加权衰减后得到
\begin{equation}
	J(\omega  ,b) = \lambda ||\omega||_2^2 - E_{x,y p_{data}} \log p_{model}(y|x)
	\label{form:5.93}
\end{equation}

这仍然保持了闭合形式的优化。

如果我们将模型变为非线性的，那么大多数代价函数就不能以闭合形式优化了。这就要求我们必须选择一种数值迭代优化方法，比如梯度下降法。

组合模型、代价和优化——这一构建学习算法的秘方同时适用于有监督学习和无监督学习。线性回归的例子体现了其对有监督学习的支持。为了支持无监督学习，可以定义一个只含$X$的数据集并提供一个合适的无监督代价函数与模型。例如，我们可以通过如下代价函数得到第一个PCA：
\begin{equation}
	J(\omega) = E_{X p_{data}}||x - r(x;\omega)||_2^2
	\label{form:5.94}
\end{equation}

同时我们的模型定义为具有模为$1$的$\omega$以及构造函数$r(x)=\omega^Tx\omega$。

在有的情况下，出于计算方面的原因，代价函数可能是无法精确衡量的。这时我们仍然可以借助数值迭代优化进行近似最小化，只要我们有途径估算其梯度。

大多数机器学习算法都利用了之一秘方，尽管有些不那么明显。如果一个机器学习算法看上去非常独特或像是手工设计的，那通常可以理解为是使用了针对特殊算例的优化器。有些模型，如决策树或k平均就是需要针对特殊算例的优化器，因为他们的代价函数中有平缓区域，使得他们不适合基于梯度的优化器。知道了绝大多数机器学习算法可以根据这一秘方来描述，这将有助于我们把不同的算法看作一个方法的分类系统：用相似的性质解决相关的问题，而不是面对一大长串毫无规律、各有特色的算法。


\section{深度学习算法的动力}
\label{sec:5.11}

本章所介绍的简单机器学习算法，在很多重要问题中都表现良好。然而，还是有很多人工智能里的核心问题他们解决不了，比如语音识别和物体识别。

深度学习的发展动力，有一部分正是来自于传统算法在这些AI任务上的失败。

这一节主要讲的是，对于高维数据，推广规律至新样本的难度指数上升；以及传统机器学习算法的机制，已经不足以在高维空间里获得普遍性。这些高维空间通常带来极高的计算开销。深度学习正是为了克服这些困难而设计的。


\subsection{维数灾难}
\label{sec:5.11.1}

当数据的维数很高的时候，许多机器学习算法也变得越来越难以实现。这一现象被称作“维数灾难”。尤其令人担忧的是，随着变量的数量增加，相应的独立组态的数量将呈指数式增长。

\begin{figure}[htbp]
   \centering
   \includegraphics[width=6in]{fig/chap5/5_9.png} 
   \caption{当数据的相关维度数量增加时（从左至右），有意义的组态数量可能会呈指数式增长。
   （左）在一维示例中，我们只有一个变量，分成10个区域；只要每隔域中有足够的样本（每个区域对应图中的一个色块），学习算法就可以轻易地正确推广。推广的最直接方式就是判断目标函数在每个区域中的值（或者进行邻域插值）。
   （中）如果有2个维度，用10个离散值就很难衡量每个变量了，我们需要$10\times 10=100$个区域和足够多的样本来覆盖所有区域。
   （右）区域的数量在3维情况下就增长到了$10^3=1000$。
   对于维数$d$和每个维度上的离散值数量$v$，我们需要$O(v^d)$的区域数和样本量。这是维数灾难的一个实例。图片由Nicolas Chapados提供。}
   \label{fig:5_9}
\end{figure}


维数灾难在很多计算科学领域中都有体现，尤其是机器学习。

维数灾难带来的一大挑战就是统计挑战。就像\ref{fig:5_9}表现的，因为可能的组态参数$x$的数量要远大于训练样本的数量。为了更好地理解这一问题，考虑把输入空间划分成网格状，如上图所示。我们可以认为仅有少量网格的低维空间几乎都被数据填充了。当需要推广到新数据的时候，我们通常只是简单地观察与新数据同在一格的训练样本，就知道接下来要怎么做了。比如，如果要预测某点$x$的概率密度，我们可以直接返回$x$所在单元网格中的样本数量，再除以训练样本总数。如果我们想要分类一个样本，我们可以返回同一单元网格中出现最多的训练样本种类。如果我们在做回归，那么我们可以对网格中所有样本取平均值。但是如果一个网格中没有任何样本，计将安出？因为在高维空间中，组态的数量极为庞大，要比训练样本多得多，所以不含任何训练样本的网格将成为常态。对于新的组态，我们能得出任何有效信息吗？很多传统机器学习算法只是简单地估计，新数据的输出应该与最近的训练点相同。

\subsection{局部守恒性与光滑正则化}
\label{sec:5.11.2}

为了更好的一般化，机器学习算法需要关于学习目标函数的先验信念(prior brief)。在前面的章节中，我们已经看过了这些显式的先验，那就是模型参数的概率分布。如果再非正式一点的话，我们也可以说先验是直接影响\textit{函数}本身并且只直接作用于参数。并且我们一般都是有倾向性地选择算法，这其实就是隐式地表达先验，哪怕这些倾向很难（或根本不可能）以概率分布的形式来表达。

使用最广泛的“先验信念”是\textbf{平滑先验}和\textbf{局部守恒先验}。这一先验指出，我们想要学习的函数，在微小的区域内不会产生很大变化。

很多简单的算法特别依赖于这一先验，才能很有效一般化，且结果是它们无法应对AI级任务的统计挑战。本书我们将讲解深度学习是如何建立（显式或隐式）先验以降低高级任务的一般化误差。这里我们先解释，为什么光滑先验不足以解决这些任务。

隐式或显式表达学习函数具有平滑先验或局部恒定的方式有很多。所有这些方法都是在鼓励学习过程去学得一个满足如下条件的函数$f^*$:
\begin{equation}
	f^*(x) \approx f^*(x + \epsilon)
	\label{form:5.95}
\end{equation}

其中$x$是大多数组态而$\epsilon$是微小的变化。换句话说，如果我们知道了输入$x$的好答案（比如$x$是有标签的训练样本）那么这个好答案很大可能也是用户$x$的临近点。如果我们在临域有多个好答案，那么我们可以整合起来（取平均值或插值）来获得一个尽可能适用于多个好答案的结果。

一个局部守恒性的极端例子是学习算法中的k近邻一族。这些预测器对训练集中拥有相同$k$个近邻的样本$x$是真正的恒定。当$k=1$，算法能区分出来的区域就不回避训练样本更多了。

k近邻算法从邻近的训练样本中复制输出，大多数核机器则是对相邻训练集的输出做插值。一类重要的核叫作\textbf{局部核(local kernel)}，当$u=v$时$k(u,v)$非常大，而当$u$和$v$远离时则减小。局部核可以看作是执行模版匹配的相似性函数，衡量一个测试样本$x$距离每个训练样本$x^(i)$有多近。很多现代深度学习开发的目的，正是为了探究局部模板匹配的极限，以及深度模型可以在局部模板匹配失效的情况下，取得怎样的成绩。

决策树也受到平滑基学习的限制影响，因为它们把输入空间分解成许多叶子一样的子域，并且在每个叶子中使用一个单独的参数（有的决策树扩展模型里，是多个参数）。如果目标函数需要至少$n$个叶子来准确表达，那么至少就得有$n$个样本来保证预测输出的统计置信度。

基本上，为了区分出输入空间的$O(k)$个区域，所有这些方法都需要$O(k)$个样本。一般也会有$O(k)$个参数，每$O(k)$个区域都有$O(1)$个参数。在近邻算法中，每个训练样本最多可以定义1个区域，如\ref{fig:5_10}所示。

\begin{figure}[htbp]
   \centering
   \includegraphics[width=6in]{fig/chap5/5_10.png} 
   \caption{近邻算法分割输入空间的示例。一个样本（表示为圆圈）定义了区域的边界（表示为直线）。每个样本的y值表示同一区域中所有点的输出。由近邻匹配定义而得的几何模式叫Voronoi图。这些相互毗邻的区域不可能比训练样本的数量增加更快。此图特别演示了近邻算法的行为，其他基于局部光滑性先验的机器学习算法也具有相似特征：每个训练样本都告诉学习算法，如何推广到该样本周围的空间。}
   \label{fig:5_10}
\end{figure}


有没有一种途径可以表征区域比训练样本还多的复杂函数呢？显然，仅仅平滑假设是不可能让学习算法做到的。比如，想象我们的目标函数像个西洋棋盘。一个西洋棋盘包含很多变化，但是存在一个简单的结构。如果训练样本的数量明显少于黑白格子的数量，会发生什么呢？基于局部一般化和平滑性（或局部守恒性），我们可以猜出一个新点的颜色，如果该点和某个训练样本位于同一格子里。但是如果一个格子里没有训练样本，谁都无法保证学习算法能够把得到的结构推广到这个格子上。基于这些先验，样本告诉我们的信息仅仅是自己所在方格的颜色，要想知道准确获知整个棋盘的颜色分布，只能让每个格子都至少含有一个样本。

只要有足够多的点，让学习算法观察到所学函数峰值附近的高点、谷值周围的低点，那么建立在光滑性假设基础上的无参数学习方法就可以表现的非常好。只要函数足够光滑，只在足够少的维度上变化，这一规律也基本都是正确的。在高维情形中，即便是非常光滑的函数也可能在每个维度上以不同的路径平滑变化，这将非常难以通过一系列训练样本来表示。如果函数相当复杂（相对于训练样本数量，我们想要区分出很多个类别），这还有希望一般化吗？

以下两个问题——能否有效表征复杂函数，能否推广函数至新输入——答案都是肯定的。数量非常多的区域，比如
$O(2^k)$，可以由$O(k)$个样本定义，只要我们根据数据生成分布的额外假设，引入区域之间的依赖性。如此便可以产生推广到局部以外(Bengio and Monperrus, 2005; Bengio et al., 2006c)。为了凸显优势，许多不同的深度学习算法都提供了隐式或显式的假设，这些假设对于很多AI任务都是合理的。

另一个优化机器学习算法的途径，是做更强、更有针对性的假设。比如，我们可以轻松的解决棋盘颜色分布问题，因为已知目标函数是周期性的（黑白相间）。通常我们不会向神经网络中引入如此强而针对的假设，所以才能推广到种类更加宽泛的结构上。有些AI任务过于复杂，无法简化到能够手工制定规律（比如周期性），因此我们希望学习算法包含更具普适性的假设。

深度学习的核心思想，就是我们假设数据都是因素或者某些特征而产生的，且具有多级层次结构。其他许多类似的假设可以进一步优化深度学习算法。这些显然很轻微的假设为样本数量和待分类区域数量之间的关系，可以带来巨大的促进。这种促进在6.4.1, 15.4和15.5等章节中还有深入描述。深度的、分布式的表征所带来的指数性优势，同样也会遇到维数灾难带来的指数性挑战。

\subsection{流形学习}
\label{sec:5.11.3}

流形（manifold）是很多机器学习原理背后的重要概念。

流形就是一个连通的区域。从数学上讲，就是一系列的点，以及每个点周围的空间。对于任何给定的点，流形都局部表现为欧几里得空间。在日常生活中，我们感觉世界是个2D平面，但实际上那是一个3D空间里的流形。

按照定义，“每个点周围的区域”表示可以从一个位置移动到临近的位置。就像在地球上，一个人可以向东西南北方向任意走动。

尽管在数学上，“流形”有着严格的定义，但在机器学习的应用中就宽松的多，只是用来描述一系列的点，这些点可以近似的认为是嵌入在高维空间中的几个自由度或维度。每个维度对应着一个变分的方向。\ref{fig:5_11}是一维训练数据嵌入在二维空间的例子。在机器学习的语境中，我们允许流形在不同点上的维数不同。这在流形与自己相交时很常见。比如把阿拉伯数字8看作一个流形，在多数位置上只有一个维度，但在中心的交叉处有两个。

\begin{figure}[htbp]
   \centering
   \includegraphics[width=6in]{fig/chap5/5_11.png} 
   \caption{从二维空间中采样得到的数据，实际上是位于一维流形上的，就像缠绕的弦。实线表示学习者应该推断出的、内在的流形}
   \label{fig:5_11}
\end{figure}

如果我们一定期望机器学习算法在所有的$R^n$域上学习出相关变量的函数，那很多问题就没有实际解决的可能了。\textbf{流形学习}假设大部分的$R^n$包含的都是无关输入，而真正令人感兴趣的输入和输出只存在于一部分流形上的一小部分点。得到这些输出变化规律的函数，也只在这些流形内的方向（或两个流形之间）有效，流形学习正是基于如此假设来克服上述困难。流形学习最早是在连续数据和无监督学习中被引入的，尽管这种基于概率的方法也可以推广到离散数据和有监督学习，但是关键假设仍是建立在质量函数高度集中的基础上。

数据存在于低维流形上的假设并非始终正确或有效。在AI任务比如图像、声音、文字的处理中，流形假设很多时候其实都不对。支持这一假设的证据主要分为两类观测。

第一类支持流形假说的观测是，现实生活中的图像、字符串、声音的概率密度是高度集中的。均匀的噪音几乎不可能形成什么结构化的输入，如\ref{fig:5_12.png}所示，均匀采样的噪音就像电视没信号时的雪花。类似的，如果随机抽取字母组成文档，形成一份有意义的文本的概率有多少？几乎可以说是0，另外，几乎所有的长字符串都是没有自然语言意义的：自然语言的集合只占全部字符串空间的极小部分。


\begin{figure}[htbp]
   \centering
   \includegraphics[width=6in]{fig/chap5/5_12.png} 
   \caption{均匀采样形成的图像（每个像素都是从均匀分布中随机选取的）都是噪点。尽管真的形成一张脸或其他物体的概率不完全是0，但有生之年我们估计都无法真正看到。这说明AI应用中所实际处理的图像只在图像空间中，占据极小的一部分。}
   \label{fig:5_12}
\end{figure}

当然集中的概率分布并不一定就能推导出数据只存在于少数流形上。我们也必须其他样本来保证样本之间是相连接的，这些样本被其他高度相似的样本所包围，并且是可以通过流形中的移动得到。第二个流形假说的支撑就是我们可以如此想象理解临域和移动，至少不那么严格地。比如对于图像问题来说，我们可以坚定地认为，有很多钟移动可以形成图像空间中的流形：我们可以逐渐地调亮或调暗光线，逐渐平移或旋转图像中的物体，逐渐改变图体表面的颜色等等。在多数应用中，可能存在不止一个流形。比如人脸图像的流形或许就无法连接到一个猫脸的图像的流形。

这些支持流形假说的思想实验传达了一些启发性的推理。更加严谨的实验(Cayton, 2005;Narayanan and Mitter, 2010; Scholkopf et al., 1998; Roweis and Saul, 2000; Tenenbawn et al., 2000; Brand, 2003; Donoho and Grimes, 2003;Weinberger and Saul, 2004)明确支持了假说在AI相关的很多数据集类型中都成立。

当数据存在于低维流形是，机器学习算法最自然的表征数据的方法是在流形的坐标系上，而不是整个$R^n$。在日常生活中，我们可以把道理看作3D空间里的1D流形。我们根据1D道路的方向定位地址，而不是在整个3D空间中。提取这些流形坐标是很难的，但是毕竟可以改进许多的机器学习算法。这一普遍原则适用于很多应用场景。\ref{fig:5_13.png}展示了人脸数据集的流形结构。在本书的最后，我们将详述发现流形结构的必要方法。在图20.6中，我们将看到一个机器学习算法如何完成这一任务。

这就是第一部分的全部内容，提供了一些数学和机器学习的基本概念，这些概念将贯穿本书的后半部分。现在，你应该已经准备好开始深度学习了。

\begin{figure}[htbp]
   \centering
   \includegraphics[width=6in]{fig/chap5/5_13.png} 
   \caption{来自QMUL Multiview Face Dataset(Gong et al., 2000)，图像中的人按照特定规则移动面部，来覆盖两个转角对应的二维流形。我们希望学习算法能够自主发现并解析出流形结构。图20.6展现出了这一成绩。}
   \label{fig:5_13}
\end{figure}
